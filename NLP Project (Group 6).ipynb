{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Monty\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Issue_id', 'Duplicated_issue', 'Title1', 'Description1', 'Title2', 'Description2', 'Label', 'Unnamed: 7', 'Unnamed: 8', 'Unnamed: 9', 'Unnamed: 10', 'Unnamed: 11', 'Unnamed: 12', 'Unnamed: 13', 'Unnamed: 14', 'Unnamed: 15', 'Unnamed: 16', 'Unnamed: 17', 'Unnamed: 18', 'Unnamed: 19', 'Unnamed: 20', 'Unnamed: 21', 'Unnamed: 22', 'Unnamed: 23', 'Unnamed: 24', 'Unnamed: 25', 'Unnamed: 26', 'Unnamed: 27', 'Unnamed: 28', 'Unnamed: 29', 'Unnamed: 30', 'Unnamed: 31', 'Unnamed: 32', 'Unnamed: 33', 'Unnamed: 34', 'Unnamed: 35', 'Unnamed: 36', 'Unnamed: 37', 'Unnamed: 38', 'Unnamed: 39', 'Unnamed: 40', 'Unnamed: 41', 'Unnamed: 42', 'Unnamed: 43', 'Unnamed: 44', 'Unnamed: 45', 'Unnamed: 46', 'Unnamed: 47', 'Unnamed: 48', 'Unnamed: 49', 'Unnamed: 50', 'Unnamed: 51', 'Unnamed: 52', 'Unnamed: 53', 'Unnamed: 54', 'Unnamed: 55', 'Unnamed: 56', 'Unnamed: 57', 'Unnamed: 58', 'Unnamed: 59', 'Unnamed: 60', 'Unnamed: 61', 'Unnamed: 62', 'Unnamed: 63', 'Unnamed: 64', 'Unnamed: 65', 'Unnamed: 66', 'Unnamed: 67', 'Unnamed: 68', 'Unnamed: 69', 'Unnamed: 70', 'Unnamed: 71', 'Unnamed: 72', 'Unnamed: 73', 'Unnamed: 74', 'Unnamed: 75', 'Unnamed: 76', 'Unnamed: 77', 'Unnamed: 78', 'Unnamed: 79', 'Unnamed: 80', 'Unnamed: 81', 'Unnamed: 82', 'Unnamed: 83', 'Unnamed: 84', 'Unnamed: 85', 'Unnamed: 86', 'Unnamed: 87', 'Unnamed: 88', 'Unnamed: 89', 'Unnamed: 90', 'Unnamed: 91', 'Unnamed: 92', 'Unnamed: 93', 'Unnamed: 94', 'Unnamed: 95', 'Unnamed: 96', 'Unnamed: 97', 'Unnamed: 98', 'Unnamed: 99', 'Unnamed: 100', 'Unnamed: 101', 'Unnamed: 102', 'Unnamed: 103', 'Unnamed: 104', 'Unnamed: 105', 'Unnamed: 106', 'Unnamed: 107', 'Unnamed: 108', 'Unnamed: 109', 'Unnamed: 110', 'Unnamed: 111', 'Unnamed: 112', 'Unnamed: 113', 'Unnamed: 114', 'Unnamed: 115', 'Unnamed: 116', 'Unnamed: 117', 'Unnamed: 118', 'Unnamed: 119', 'Unnamed: 120', 'Unnamed: 121', 'Unnamed: 122', 'Unnamed: 123', 'Unnamed: 124', 'Unnamed: 125', 'Unnamed: 126', 'Unnamed: 127', 'Unnamed: 128', 'Unnamed: 129', 'Unnamed: 130', 'Unnamed: 131', 'Unnamed: 132', 'Unnamed: 133', 'Unnamed: 134', 'Unnamed: 135', 'Unnamed: 136', 'Unnamed: 137', 'Unnamed: 138', 'Unnamed: 139', 'Unnamed: 140', 'Unnamed: 141', 'Unnamed: 142', 'Unnamed: 143', 'Unnamed: 144', 'Unnamed: 145', 'Unnamed: 146', 'Unnamed: 147', 'Unnamed: 148', 'Unnamed: 149', 'Unnamed: 150', 'Unnamed: 151', 'Unnamed: 152', 'Unnamed: 153', 'Unnamed: 154', 'Unnamed: 155', 'Unnamed: 156', 'Unnamed: 157', 'Unnamed: 158', 'Unnamed: 159', 'Unnamed: 160', 'Unnamed: 161', 'Unnamed: 162', 'Unnamed: 163', 'Unnamed: 164', 'Unnamed: 165', 'Unnamed: 166', 'Unnamed: 167', 'Unnamed: 168', 'Unnamed: 169', 'Unnamed: 170', 'Unnamed: 171', 'Unnamed: 172', 'Unnamed: 173', 'Unnamed: 174', 'Unnamed: 175', 'Unnamed: 176', 'Unnamed: 177', 'Unnamed: 178', 'Unnamed: 179', 'Unnamed: 180', 'Unnamed: 181', 'Unnamed: 182', 'Unnamed: 183', 'Unnamed: 184', 'Unnamed: 185', 'Unnamed: 186', 'Unnamed: 187', 'Unnamed: 188', 'Unnamed: 189', 'Unnamed: 190', 'Unnamed: 191', 'Unnamed: 192', 'Unnamed: 193', 'Unnamed: 194', 'Unnamed: 195', 'Unnamed: 196', 'Unnamed: 197', 'Unnamed: 198', 'Unnamed: 199', 'Unnamed: 200', 'Unnamed: 201', 'Unnamed: 202', 'Unnamed: 203', 'Unnamed: 204', 'Unnamed: 205', 'Unnamed: 206', 'Unnamed: 207', 'Unnamed: 208', 'Unnamed: 209', 'Unnamed: 210', 'Unnamed: 211', 'Unnamed: 212', 'Unnamed: 213', 'Unnamed: 214', 'Unnamed: 215', 'Unnamed: 216', 'Unnamed: 217', 'Unnamed: 218', 'Unnamed: 219', 'Unnamed: 220', 'Unnamed: 221', 'Unnamed: 222', 'Unnamed: 223', 'Unnamed: 224', 'Unnamed: 225', 'Unnamed: 226', 'Unnamed: 227', 'Unnamed: 228', 'Unnamed: 229', 'Unnamed: 230', 'Unnamed: 231', 'Unnamed: 232', 'Unnamed: 233', 'Unnamed: 234', 'Unnamed: 235', 'Unnamed: 236', 'Unnamed: 237', 'Unnamed: 238', 'Unnamed: 239', 'Unnamed: 240', 'Unnamed: 241', 'Unnamed: 242', 'Unnamed: 243', 'Unnamed: 244', 'Unnamed: 245', 'Unnamed: 246', 'Unnamed: 247', 'Unnamed: 248', 'Unnamed: 249', 'Unnamed: 250', 'Unnamed: 251', 'Unnamed: 252', 'Unnamed: 253', 'Unnamed: 254', 'Unnamed: 255', 'Unnamed: 256', 'Unnamed: 257', 'Unnamed: 258', 'Unnamed: 259', 'Unnamed: 260', 'Unnamed: 261', 'Unnamed: 262', 'Unnamed: 263', 'Unnamed: 264', 'Unnamed: 265', 'Unnamed: 266', 'Unnamed: 267', 'Unnamed: 268', 'Unnamed: 269', 'Unnamed: 270', 'Unnamed: 271', 'Unnamed: 272', 'Unnamed: 273', 'Unnamed: 274', 'Unnamed: 275', 'Unnamed: 276', 'Unnamed: 277', 'Unnamed: 278', 'Unnamed: 279', 'Unnamed: 280', 'Unnamed: 281', 'Unnamed: 282', 'Unnamed: 283', 'Unnamed: 284', 'Unnamed: 285', 'Unnamed: 286', 'Unnamed: 287', 'Unnamed: 288', 'Unnamed: 289', 'Unnamed: 290', 'Unnamed: 291', 'Unnamed: 292', 'Unnamed: 293', 'Unnamed: 294', 'Unnamed: 295', 'Unnamed: 296', 'Unnamed: 297', 'Unnamed: 298', 'Unnamed: 299', 'Unnamed: 300', 'Unnamed: 301', 'Unnamed: 302', 'Unnamed: 303', 'Unnamed: 304', 'Unnamed: 305', 'Unnamed: 306', 'Unnamed: 307', 'Unnamed: 308', 'Unnamed: 309', 'Unnamed: 310', 'Unnamed: 311', 'Unnamed: 312', 'Unnamed: 313', 'Unnamed: 314', 'Unnamed: 315', 'Unnamed: 316', 'Unnamed: 317', 'Unnamed: 318', 'Unnamed: 319', 'Unnamed: 320', 'Unnamed: 321', 'Unnamed: 322', 'Unnamed: 323', 'Unnamed: 324', 'Unnamed: 325', 'Unnamed: 326', 'Unnamed: 327', 'Unnamed: 328', 'Unnamed: 329', 'Unnamed: 330', 'Unnamed: 331', 'Unnamed: 332', 'Unnamed: 333', 'Unnamed: 334', 'Unnamed: 335', 'Unnamed: 336', 'Unnamed: 337', 'Unnamed: 338', 'Unnamed: 339', 'Unnamed: 340', 'Unnamed: 341', 'Unnamed: 342', 'Unnamed: 343', 'Unnamed: 344', 'Unnamed: 345', 'Unnamed: 346', 'Unnamed: 347', 'Unnamed: 348', 'Unnamed: 349', 'Unnamed: 350', 'Unnamed: 351', 'Unnamed: 352', 'Unnamed: 353', 'Unnamed: 354', 'Unnamed: 355', 'Unnamed: 356', 'Unnamed: 357', 'Unnamed: 358', 'Unnamed: 359', 'Unnamed: 360', 'Unnamed: 361', 'Unnamed: 362', 'Unnamed: 363', 'Unnamed: 364', 'Unnamed: 365', 'Unnamed: 366', 'Unnamed: 367', 'Unnamed: 368', 'Unnamed: 369', 'Unnamed: 370', 'Unnamed: 371', 'Unnamed: 372', 'Unnamed: 373', 'Unnamed: 374', 'Unnamed: 375', 'Unnamed: 376', 'Unnamed: 377', 'Unnamed: 378', 'Unnamed: 379', 'Unnamed: 380', 'Unnamed: 381', 'Unnamed: 382', 'Unnamed: 383', 'Unnamed: 384', 'Unnamed: 385', 'Unnamed: 386', 'Unnamed: 387', 'Unnamed: 388', 'Unnamed: 389', 'Unnamed: 390', 'Unnamed: 391', 'Unnamed: 392', 'Unnamed: 393', 'Unnamed: 394', 'Unnamed: 395', 'Unnamed: 396', 'Unnamed: 397', 'Unnamed: 398', 'Unnamed: 399', 'Unnamed: 400', 'Unnamed: 401', 'Unnamed: 402', 'Unnamed: 403', 'Unnamed: 404', 'Unnamed: 405', 'Unnamed: 406', 'Unnamed: 407', 'Unnamed: 408', 'Unnamed: 409', 'Unnamed: 410', 'Unnamed: 411', 'Unnamed: 412', 'Unnamed: 413', 'Unnamed: 414', 'Unnamed: 415', 'Unnamed: 416', 'Unnamed: 417', 'Unnamed: 418', 'Unnamed: 419', 'Unnamed: 420', 'Unnamed: 421', 'Unnamed: 422', 'Unnamed: 423', 'Unnamed: 424', 'Unnamed: 425', 'Unnamed: 426', 'Unnamed: 427', 'Unnamed: 428', 'Unnamed: 429', 'Unnamed: 430', 'Unnamed: 431', 'Unnamed: 432', 'Unnamed: 433', 'Unnamed: 434', 'Unnamed: 435', 'Unnamed: 436', 'Unnamed: 437', 'Unnamed: 438', 'Unnamed: 439', 'Unnamed: 440', 'Unnamed: 441', 'Unnamed: 442', 'Unnamed: 443', 'Unnamed: 444', 'Unnamed: 445', 'Unnamed: 446', 'Unnamed: 447', 'Unnamed: 448', 'Unnamed: 449', 'Unnamed: 450', 'Unnamed: 451', 'Unnamed: 452', 'Unnamed: 453', 'Unnamed: 454', 'Unnamed: 455', 'Unnamed: 456', 'Unnamed: 457', 'Unnamed: 458', 'Unnamed: 459', 'Unnamed: 460', 'Unnamed: 461', 'Unnamed: 462', 'Unnamed: 463', 'Unnamed: 464', 'Unnamed: 465', 'Unnamed: 466', 'Unnamed: 467', 'Unnamed: 468', 'Unnamed: 469', 'Unnamed: 470', 'Unnamed: 471', 'Unnamed: 472', 'Unnamed: 473', 'Unnamed: 474', 'Unnamed: 475', 'Unnamed: 476', 'Unnamed: 477', 'Unnamed: 478', 'Unnamed: 479', 'Unnamed: 480', 'Unnamed: 481', 'Unnamed: 482', 'Unnamed: 483', 'Unnamed: 484', 'Unnamed: 485', 'Unnamed: 486', 'Unnamed: 487', 'Unnamed: 488', 'Unnamed: 489', 'Unnamed: 490', 'Unnamed: 491', 'Unnamed: 492', 'Unnamed: 493', 'Unnamed: 494', 'Unnamed: 495', 'Unnamed: 496', 'Unnamed: 497', 'Unnamed: 498', 'Unnamed: 499', 'Unnamed: 500', 'Unnamed: 501', 'Unnamed: 502', 'Unnamed: 503', 'Unnamed: 504', 'Unnamed: 505', 'Unnamed: 506', 'Unnamed: 507', 'Unnamed: 508', 'Unnamed: 509', 'Unnamed: 510', 'Unnamed: 511', 'Unnamed: 512', 'Unnamed: 513', 'Unnamed: 514', 'Unnamed: 515', 'Unnamed: 516', 'Unnamed: 517', 'Unnamed: 518', 'Unnamed: 519', 'Unnamed: 520', 'Unnamed: 521', 'Unnamed: 522', 'Unnamed: 523', 'Unnamed: 524', 'Unnamed: 525', 'Unnamed: 526', 'Unnamed: 527', 'Unnamed: 528', 'Unnamed: 529', 'Unnamed: 530', 'Unnamed: 531', 'Unnamed: 532', 'Unnamed: 533', 'Unnamed: 534', 'Unnamed: 535', 'Unnamed: 536', 'Unnamed: 537', 'Unnamed: 538', 'Unnamed: 539', 'Unnamed: 540', 'Unnamed: 541', 'Unnamed: 542', 'Unnamed: 543', 'Unnamed: 544', 'Unnamed: 545', 'Unnamed: 546', 'Unnamed: 547', 'Unnamed: 548', 'Unnamed: 549', 'Unnamed: 550', 'Unnamed: 551', 'Unnamed: 552', 'Unnamed: 553', 'Unnamed: 554', 'Unnamed: 555', 'Unnamed: 556', 'Unnamed: 557', 'Unnamed: 558', 'Unnamed: 559', 'Unnamed: 560', 'Unnamed: 561', 'Unnamed: 562', 'Unnamed: 563', 'Unnamed: 564', 'Unnamed: 565', 'Unnamed: 566', 'Unnamed: 567', 'Unnamed: 568', 'Unnamed: 569', 'Unnamed: 570', 'Unnamed: 571', 'Unnamed: 572', 'Unnamed: 573', 'Unnamed: 574', 'Unnamed: 575', 'Unnamed: 576', 'Unnamed: 577', 'Unnamed: 578', 'Unnamed: 579', 'Unnamed: 580', 'Unnamed: 581', 'Unnamed: 582', 'Unnamed: 583', 'Unnamed: 584', 'Unnamed: 585', 'Unnamed: 586', 'Unnamed: 587', 'Unnamed: 588', 'Unnamed: 589', 'Unnamed: 590', 'Unnamed: 591', 'Unnamed: 592', 'Unnamed: 593', 'Unnamed: 594', 'Unnamed: 595', 'Unnamed: 596', 'Unnamed: 597', 'Unnamed: 598', 'Unnamed: 599', 'Unnamed: 600', 'Unnamed: 601', 'Unnamed: 602', 'Unnamed: 603', 'Unnamed: 604', 'Unnamed: 605', 'Unnamed: 606', 'Unnamed: 607', 'Unnamed: 608', 'Unnamed: 609', 'Unnamed: 610', 'Unnamed: 611', 'Unnamed: 612', 'Unnamed: 613', 'Unnamed: 614', 'Unnamed: 615', 'Unnamed: 616', 'Unnamed: 617', 'Unnamed: 618', 'Unnamed: 619', 'Unnamed: 620', 'Unnamed: 621', 'Unnamed: 622', 'Unnamed: 623', 'Unnamed: 624', 'Unnamed: 625', 'Unnamed: 626', 'Unnamed: 627', 'Unnamed: 628', 'Unnamed: 629', 'Unnamed: 630', 'Unnamed: 631', 'Unnamed: 632', 'Unnamed: 633', 'Unnamed: 634', 'Unnamed: 635', 'Unnamed: 636', 'Unnamed: 637', 'Unnamed: 638', 'Unnamed: 639', 'Unnamed: 640', 'Unnamed: 641', 'Unnamed: 642', 'Unnamed: 643', 'Unnamed: 644', 'Unnamed: 645', 'Unnamed: 646', 'Unnamed: 647', 'Unnamed: 648', 'Unnamed: 649', 'Unnamed: 650', 'Unnamed: 651', 'Unnamed: 652', 'Unnamed: 653', 'Unnamed: 654', 'Unnamed: 655', 'Unnamed: 656', 'Unnamed: 657', 'Unnamed: 658', 'Unnamed: 659', 'Unnamed: 660', 'Unnamed: 661', 'Unnamed: 662', 'Unnamed: 663', 'Unnamed: 664', 'Unnamed: 665', 'Unnamed: 666', 'Unnamed: 667', 'Unnamed: 668', 'Unnamed: 669', 'Unnamed: 670', 'Unnamed: 671', 'Unnamed: 672', 'Unnamed: 673', 'Unnamed: 674', 'Unnamed: 675', 'Unnamed: 676', 'Unnamed: 677', 'Unnamed: 678', 'Unnamed: 679', 'Unnamed: 680', 'Unnamed: 681', 'Unnamed: 682', 'Unnamed: 683', 'Unnamed: 684', 'Unnamed: 685', 'Unnamed: 686', 'Unnamed: 687', 'Unnamed: 688', 'Unnamed: 689', 'Unnamed: 690', 'Unnamed: 691', 'Unnamed: 692', 'Unnamed: 693', 'Unnamed: 694', 'Unnamed: 695', 'Unnamed: 696', 'Unnamed: 697', 'Unnamed: 698', 'Unnamed: 699', 'Unnamed: 700', 'Unnamed: 701', 'Unnamed: 702', 'Unnamed: 703', 'Unnamed: 704', 'Unnamed: 705', 'Unnamed: 706', 'Unnamed: 707', 'Unnamed: 708', 'Unnamed: 709', 'Unnamed: 710', 'Unnamed: 711', 'Unnamed: 712', 'Unnamed: 713', 'Unnamed: 714', 'Unnamed: 715', 'Unnamed: 716', 'Unnamed: 717', 'Unnamed: 718', 'Unnamed: 719', 'Unnamed: 720', 'Unnamed: 721', 'Unnamed: 722', 'Unnamed: 723', 'Unnamed: 724', 'Unnamed: 725', 'Unnamed: 726', 'Unnamed: 727', 'Unnamed: 728', 'Unnamed: 729', 'Unnamed: 730', 'Unnamed: 731', 'Unnamed: 732', 'Unnamed: 733', 'Unnamed: 734', 'Unnamed: 735', 'Unnamed: 736', 'Unnamed: 737', 'Unnamed: 738', 'Unnamed: 739', 'Unnamed: 740', 'Unnamed: 741', 'Unnamed: 742', 'Unnamed: 743', 'Unnamed: 744', 'Unnamed: 745', 'Unnamed: 746', 'Unnamed: 747', 'Unnamed: 748', 'Unnamed: 749', 'Unnamed: 750', 'Unnamed: 751', 'Unnamed: 752', 'Unnamed: 753', 'Unnamed: 754', 'Unnamed: 755', 'Unnamed: 756', 'Unnamed: 757', 'Unnamed: 758', 'Unnamed: 759', 'Unnamed: 760', 'Unnamed: 761', 'Unnamed: 762', 'Unnamed: 763', 'Unnamed: 764', 'Unnamed: 765', 'Unnamed: 766', 'Unnamed: 767', 'Unnamed: 768', 'Unnamed: 769', 'Unnamed: 770', 'Unnamed: 771', 'Unnamed: 772', 'Unnamed: 773', 'Unnamed: 774', 'Unnamed: 775', 'Unnamed: 776', 'Unnamed: 777', 'Unnamed: 778', 'Unnamed: 779', 'Unnamed: 780', 'Unnamed: 781', 'Unnamed: 782', 'Unnamed: 783', 'Unnamed: 784', 'Unnamed: 785', 'Unnamed: 786', 'Unnamed: 787', 'Unnamed: 788', 'Unnamed: 789', 'Unnamed: 790', 'Unnamed: 791', 'Unnamed: 792', 'Unnamed: 793', 'Unnamed: 794', 'Unnamed: 795', 'Unnamed: 796', 'Unnamed: 797', 'Unnamed: 798', 'Unnamed: 799', 'Unnamed: 800', 'Unnamed: 801', 'Unnamed: 802']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "import nltk.corpus\n",
    "import pandas as pd\n",
    "import string\n",
    "import re\n",
    "\n",
    "import multiprocessing\n",
    "import gensim\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "data_df = pd.read_csv('./data/Mozilla/M_Duplicate BRs.csv')\n",
    "data_df.head()\n",
    "print (list(data_df))\n",
    "\n",
    "data_df['Description1']=data_df['Description1'].fillna('').apply(str)\n",
    "data_df['Description2']=data_df['Description2'].fillna('').apply(str)\n",
    "data_df['Title1']=data_df['Title1'].fillna('').apply(str)\n",
    "data_df['Title2']=data_df['Title2'].fillna('').apply(str)\n",
    "\n",
    "data_df['Label'] = (data_df['Label'] =='1.00').astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Issue_id', 'Duplicated_issue', 'Title1', 'Description1', 'Title2', 'Description2', 'Label', 'Unnamed: 7', 'Unnamed: 8', 'Unnamed: 9', 'Unnamed: 10', 'Unnamed: 11', 'Unnamed: 12', 'Unnamed: 13', 'Unnamed: 14', 'Unnamed: 15', 'Unnamed: 16', 'Unnamed: 17', 'Unnamed: 18', 'Unnamed: 19', 'Unnamed: 20', 'Unnamed: 21', 'Unnamed: 22', 'Unnamed: 23', 'Unnamed: 24', 'Unnamed: 25', 'Unnamed: 26', 'Unnamed: 27', 'Unnamed: 28', 'Unnamed: 29', 'Unnamed: 30', 'Unnamed: 31', 'Unnamed: 32', 'Unnamed: 33', 'Unnamed: 34', 'Unnamed: 35', 'Unnamed: 36', 'Unnamed: 37', 'Unnamed: 38', 'Unnamed: 39', 'Unnamed: 40', 'Unnamed: 41', 'Unnamed: 42', 'Unnamed: 43', 'Unnamed: 44', 'Unnamed: 45', 'Unnamed: 46', 'Unnamed: 47', 'Unnamed: 48', 'Unnamed: 49', 'Unnamed: 50', 'Unnamed: 51', 'Unnamed: 52', 'Unnamed: 53', 'Unnamed: 54', 'Unnamed: 55', 'Unnamed: 56', 'Unnamed: 57', 'Unnamed: 58', 'Unnamed: 59', 'Unnamed: 60', 'Unnamed: 61', 'Unnamed: 62', 'Unnamed: 63', 'Unnamed: 64', 'Unnamed: 65', 'Unnamed: 66', 'Unnamed: 67', 'Unnamed: 68', 'Unnamed: 69', 'Unnamed: 70', 'Unnamed: 71', 'Unnamed: 72', 'Unnamed: 73', 'Unnamed: 74', 'Unnamed: 75', 'Unnamed: 76', 'Unnamed: 77', 'Unnamed: 78', 'Unnamed: 79', 'Unnamed: 80', 'Unnamed: 81', 'Unnamed: 82', 'Unnamed: 83', 'Unnamed: 84', 'Unnamed: 85', 'Unnamed: 86', 'Unnamed: 87', 'Unnamed: 88', 'Unnamed: 89', 'Unnamed: 90', 'Unnamed: 91', 'Unnamed: 92', 'Unnamed: 93', 'Unnamed: 94', 'Unnamed: 95', 'Unnamed: 96', 'Unnamed: 97', 'Unnamed: 98', 'Unnamed: 99', 'Unnamed: 100', 'Unnamed: 101', 'Unnamed: 102', 'Unnamed: 103', 'Unnamed: 104', 'Unnamed: 105', 'Unnamed: 106', 'Unnamed: 107', 'Unnamed: 108', 'Unnamed: 109', 'Unnamed: 110', 'Unnamed: 111', 'Unnamed: 112', 'Unnamed: 113', 'Unnamed: 114', 'Unnamed: 115', 'Unnamed: 116', 'Unnamed: 117', 'Unnamed: 118', 'Unnamed: 119', 'Unnamed: 120', 'Unnamed: 121', 'Unnamed: 122', 'Unnamed: 123', 'Unnamed: 124', 'Unnamed: 125', 'Unnamed: 126', 'Unnamed: 127', 'Unnamed: 128', 'Unnamed: 129', 'Unnamed: 130', 'Unnamed: 131', 'Unnamed: 132', 'Unnamed: 133', 'Unnamed: 134', 'Unnamed: 135', 'Unnamed: 136', 'Unnamed: 137', 'Unnamed: 138', 'Unnamed: 139', 'Unnamed: 140', 'Unnamed: 141', 'Unnamed: 142', 'Unnamed: 143', 'Unnamed: 144', 'Unnamed: 145', 'Unnamed: 146', 'Unnamed: 147', 'Unnamed: 148', 'Unnamed: 149', 'Unnamed: 150', 'Unnamed: 151', 'Unnamed: 152', 'Unnamed: 153', 'Unnamed: 154', 'Unnamed: 155', 'Unnamed: 156', 'Unnamed: 157', 'Unnamed: 158', 'Unnamed: 159', 'Unnamed: 160', 'Unnamed: 161', 'Unnamed: 162', 'Unnamed: 163', 'Unnamed: 164', 'Unnamed: 165', 'Unnamed: 166', 'Unnamed: 167', 'Unnamed: 168', 'Unnamed: 169', 'Unnamed: 170', 'Unnamed: 171', 'Unnamed: 172', 'Unnamed: 173', 'Unnamed: 174', 'Unnamed: 175', 'Unnamed: 176', 'Unnamed: 177', 'Unnamed: 178', 'Unnamed: 179', 'Unnamed: 180', 'Unnamed: 181', 'Unnamed: 182', 'Unnamed: 183', 'Unnamed: 184', 'Unnamed: 185', 'Unnamed: 186', 'Unnamed: 187', 'Unnamed: 188', 'Unnamed: 189', 'Unnamed: 190', 'Unnamed: 191', 'Unnamed: 192', 'Unnamed: 193', 'Unnamed: 194', 'Unnamed: 195', 'Unnamed: 196', 'Unnamed: 197', 'Unnamed: 198', 'Unnamed: 199', 'Unnamed: 200', 'Unnamed: 201', 'Unnamed: 202', 'Unnamed: 203', 'Unnamed: 204', 'Unnamed: 205', 'Unnamed: 206', 'Unnamed: 207', 'Unnamed: 208', 'Unnamed: 209', 'Unnamed: 210', 'Unnamed: 211', 'Unnamed: 212', 'Unnamed: 213', 'Unnamed: 214', 'Unnamed: 215', 'Unnamed: 216', 'Unnamed: 217', 'Unnamed: 218', 'Unnamed: 219', 'Unnamed: 220', 'Unnamed: 221', 'Unnamed: 222', 'Unnamed: 223', 'Unnamed: 224', 'Unnamed: 225', 'Unnamed: 226', 'Unnamed: 227', 'Unnamed: 228', 'Unnamed: 229', 'Unnamed: 230', 'Unnamed: 231', 'Unnamed: 232', 'Unnamed: 233', 'Unnamed: 234', 'Unnamed: 235', 'Unnamed: 236', 'Unnamed: 237', 'Unnamed: 238', 'Unnamed: 239', 'Unnamed: 240', 'Unnamed: 241', 'Unnamed: 242', 'Unnamed: 243', 'Unnamed: 244', 'Unnamed: 245', 'Unnamed: 246', 'Unnamed: 247', 'Unnamed: 248', 'Unnamed: 249', 'Unnamed: 250', 'Unnamed: 251', 'Unnamed: 252', 'Unnamed: 253', 'Unnamed: 254', 'Unnamed: 255', 'Unnamed: 256', 'Unnamed: 257', 'Unnamed: 258', 'Unnamed: 259', 'Unnamed: 260', 'Unnamed: 261', 'Unnamed: 262', 'Unnamed: 263', 'Unnamed: 264', 'Unnamed: 265', 'Unnamed: 266', 'Unnamed: 267', 'Unnamed: 268', 'Unnamed: 269', 'Unnamed: 270', 'Unnamed: 271', 'Unnamed: 272', 'Unnamed: 273', 'Unnamed: 274', 'Unnamed: 275', 'Unnamed: 276', 'Unnamed: 277', 'Unnamed: 278', 'Unnamed: 279', 'Unnamed: 280', 'Unnamed: 281', 'Unnamed: 282', 'Unnamed: 283', 'Unnamed: 284', 'Unnamed: 285', 'Unnamed: 286', 'Unnamed: 287', 'Unnamed: 288', 'Unnamed: 289', 'Unnamed: 290', 'Unnamed: 291', 'Unnamed: 292', 'Unnamed: 293', 'Unnamed: 294', 'Unnamed: 295', 'Unnamed: 296', 'Unnamed: 297', 'Unnamed: 298', 'Unnamed: 299', 'Unnamed: 300', 'Unnamed: 301', 'Unnamed: 302', 'Unnamed: 303', 'Unnamed: 304', 'Unnamed: 305', 'Unnamed: 306', 'Unnamed: 307', 'Unnamed: 308', 'Unnamed: 309', 'Unnamed: 310', 'Unnamed: 311', 'Unnamed: 312', 'Unnamed: 313', 'Unnamed: 314', 'Unnamed: 315', 'Unnamed: 316', 'Unnamed: 317', 'Unnamed: 318', 'Unnamed: 319', 'Unnamed: 320', 'Unnamed: 321', 'Unnamed: 322', 'Unnamed: 323', 'Unnamed: 324', 'Unnamed: 325', 'Unnamed: 326', 'Unnamed: 327', 'Unnamed: 328', 'Unnamed: 329', 'Unnamed: 330', 'Unnamed: 331', 'Unnamed: 332', 'Unnamed: 333', 'Unnamed: 334', 'Unnamed: 335', 'Unnamed: 336', 'Unnamed: 337', 'Unnamed: 338', 'Unnamed: 339', 'Unnamed: 340', 'Unnamed: 341', 'Unnamed: 342', 'Unnamed: 343', 'Unnamed: 344', 'Unnamed: 345', 'Unnamed: 346', 'Unnamed: 347', 'Unnamed: 348', 'Unnamed: 349', 'Unnamed: 350', 'Unnamed: 351', 'Unnamed: 352', 'Unnamed: 353', 'Unnamed: 354', 'Unnamed: 355', 'Unnamed: 356', 'Unnamed: 357', 'Unnamed: 358', 'Unnamed: 359', 'Unnamed: 360', 'Unnamed: 361', 'Unnamed: 362', 'Unnamed: 363', 'Unnamed: 364', 'Unnamed: 365', 'Unnamed: 366', 'Unnamed: 367', 'Unnamed: 368', 'Unnamed: 369', 'Unnamed: 370', 'Unnamed: 371', 'Unnamed: 372', 'Unnamed: 373', 'Unnamed: 374', 'Unnamed: 375', 'Unnamed: 376', 'Unnamed: 377', 'Unnamed: 378', 'Unnamed: 379', 'Unnamed: 380', 'Unnamed: 381', 'Unnamed: 382', 'Unnamed: 383', 'Unnamed: 384', 'Unnamed: 385', 'Unnamed: 386', 'Unnamed: 387', 'Unnamed: 388', 'Unnamed: 389', 'Unnamed: 390', 'Unnamed: 391', 'Unnamed: 392', 'Unnamed: 393', 'Unnamed: 394', 'Unnamed: 395', 'Unnamed: 396', 'Unnamed: 397', 'Unnamed: 398', 'Unnamed: 399', 'Unnamed: 400', 'Unnamed: 401', 'Unnamed: 402', 'Unnamed: 403', 'Unnamed: 404', 'Unnamed: 405', 'Unnamed: 406', 'Unnamed: 407', 'Unnamed: 408', 'Unnamed: 409', 'Unnamed: 410', 'Unnamed: 411', 'Unnamed: 412', 'Unnamed: 413', 'Unnamed: 414', 'Unnamed: 415', 'Unnamed: 416', 'Unnamed: 417', 'Unnamed: 418', 'Unnamed: 419', 'Unnamed: 420', 'Unnamed: 421', 'Unnamed: 422', 'Unnamed: 423', 'Unnamed: 424', 'Unnamed: 425', 'Unnamed: 426', 'Unnamed: 427', 'Unnamed: 428']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0        the dialup properties of the profile should be...\n",
       "1        i would really like a plugin manager for my br...\n",
       "2        language encodings are listed in a seemingly r...\n",
       "3        i am using a synaptics touch pad with the late...\n",
       "4        the history window should select and scroll to...\n",
       "                               ...                        \n",
       "36828    i have a file based homepage.  when i click on...\n",
       "36829    whilst testing something which required me to ...\n",
       "36830    sorry for not having an example of this the we...\n",
       "36831    fixdevtoolswarnings.patch  enabling pref javas...\n",
       "36832    partwarnings.patch  enabling pref javascript.o...\n",
       "Name: Description1, Length: 36833, dtype: object"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nodup_df = pd.read_csv('./data/Mozilla/M_NonDuplicate BRs.csv',encoding='cp1252')\n",
    "nodup_df.head()\n",
    "print (list(nodup_df))\n",
    "\n",
    "nodup_df['Description1']=nodup_df['Description1'].fillna('').apply(str)\n",
    "nodup_df['Description2']=nodup_df['Description2'].fillna('').apply(str)\n",
    "nodup_df['Title1']=nodup_df['Title1'].fillna('').apply(str)\n",
    "nodup_df['Title2']=nodup_df['Title2'].fillna('').apply(str)\n",
    "\n",
    "nodup_df['Description1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import re\n",
    "# stop_words = stopwords.words('english')\n",
    "# print(stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def identify_tokens(row):\n",
    "    review = row['Description1']\n",
    "    tokens = nltk.word_tokenize(review)\n",
    "    token_words = [w for w in tokens if w.isalpha()]\n",
    "    return token_words\n",
    "\n",
    "data_df['Description1_clean'] = data_df.apply(identify_tokens, axis=1)\n",
    "nodup_df['Description1_clean'] = nodup_df.apply(identify_tokens, axis=1)\n",
    "\n",
    "def identify_tokens(row):\n",
    "    review = row['Description2']\n",
    "    tokens = nltk.word_tokenize(review)\n",
    "    token_words = [w for w in tokens if w.isalpha()]\n",
    "    return token_words\n",
    "\n",
    "data_df['Description2_clean'] = data_df.apply(identify_tokens, axis=1)\n",
    "nodup_df['Description2_clean'] = nodup_df.apply(identify_tokens, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['in',\n",
       " 'the',\n",
       " 'example',\n",
       " 'page',\n",
       " 'see',\n",
       " 'url',\n",
       " 'i',\n",
       " 'have',\n",
       " 'tried',\n",
       " 'to',\n",
       " 'vertically',\n",
       " 'align',\n",
       " 'all',\n",
       " 'the',\n",
       " 'cells',\n",
       " 'using',\n",
       " 'col',\n",
       " 'valigntop',\n",
       " 'mac',\n",
       " 'mozilla',\n",
       " 'and',\n",
       " 'win',\n",
       " 'firefox',\n",
       " 'both',\n",
       " 'demonstrate',\n",
       " 'this',\n",
       " 'problem',\n",
       " 'so',\n",
       " 'it',\n",
       " 'seems',\n",
       " 'to',\n",
       " 'be',\n",
       " 'quite',\n",
       " 'an',\n",
       " 'old',\n",
       " 'issue',\n",
       " 'it',\n",
       " 'might',\n",
       " 'be',\n",
       " 'related',\n",
       " 'to',\n",
       " 'bug',\n",
       " 'though',\n",
       " 'i',\n",
       " 'dont',\n",
       " 'believe',\n",
       " 'that',\n",
       " 'dealt',\n",
       " 'with',\n",
       " 'valign',\n",
       " 'the',\n",
       " 'page',\n",
       " 'also',\n",
       " 'shows',\n",
       " 'that',\n",
       " 'col',\n",
       " 'wont',\n",
       " 'apply',\n",
       " 'background',\n",
       " 'colours',\n",
       " 'to',\n",
       " 'cells',\n",
       " 'either',\n",
       " 'as',\n",
       " 'well',\n",
       " 'as',\n",
       " 'not',\n",
       " 'supporting',\n",
       " 'valign',\n",
       " 'the',\n",
       " 'whole',\n",
       " 'left',\n",
       " 'column',\n",
       " 'is',\n",
       " 'meant',\n",
       " 'to',\n",
       " 'be',\n",
       " 'green',\n",
       " 'but',\n",
       " 'mozfirefox',\n",
       " 'wont',\n",
       " 'display',\n",
       " 'the',\n",
       " 'colour',\n",
       " 'this',\n",
       " 'is',\n",
       " 'possibly',\n",
       " 'bug',\n",
       " 'i',\n",
       " 'have',\n",
       " 'tried',\n",
       " 'to',\n",
       " 'use',\n",
       " 'css',\n",
       " 'instead',\n",
       " 'using',\n",
       " 'verticalalign',\n",
       " 'instead',\n",
       " 'of',\n",
       " 'valign',\n",
       " 'i',\n",
       " 'tried',\n",
       " 'applying',\n",
       " 'styleverticalalign',\n",
       " 'top',\n",
       " 'to',\n",
       " 'the',\n",
       " 'col',\n",
       " 'tags',\n",
       " 'as',\n",
       " 'a',\n",
       " 'workaround',\n",
       " 'in',\n",
       " 'case',\n",
       " 'this',\n",
       " 'issue',\n",
       " 'is',\n",
       " 'due',\n",
       " 'to',\n",
       " 'a',\n",
       " 'lack',\n",
       " 'of',\n",
       " 'support',\n",
       " 'for',\n",
       " 'outdated',\n",
       " 'html',\n",
       " 'properties',\n",
       " 'and',\n",
       " 'this',\n",
       " 'did',\n",
       " 'not',\n",
       " 'work',\n",
       " 'either',\n",
       " 'col',\n",
       " 'seems',\n",
       " 'to',\n",
       " 'not',\n",
       " 'apply',\n",
       " 'any',\n",
       " 'formatting',\n",
       " 'to',\n",
       " 'cells',\n",
       " 'whatsoever',\n",
       " 'neither',\n",
       " 'css',\n",
       " 'nor',\n",
       " 'html',\n",
       " 'microsoft',\n",
       " 'ie',\n",
       " 'for',\n",
       " 'the',\n",
       " 'macintosh',\n",
       " 'also',\n",
       " 'displays',\n",
       " 'the',\n",
       " 'valign',\n",
       " 'issue',\n",
       " 'no',\n",
       " 'support',\n",
       " 'for',\n",
       " 'either',\n",
       " 'form',\n",
       " 'of',\n",
       " 'vertical',\n",
       " 'alignment',\n",
       " 'in',\n",
       " 'col',\n",
       " 'but',\n",
       " 'icab',\n",
       " 'b',\n",
       " 'mac',\n",
       " 'browser',\n",
       " 'handles',\n",
       " 'valign',\n",
       " 'correctly',\n",
       " 'not',\n",
       " 'tried',\n",
       " 'it',\n",
       " 'with',\n",
       " 'verticalalign',\n",
       " 'as',\n",
       " 'its',\n",
       " 'css',\n",
       " 'support',\n",
       " 'is',\n",
       " 'lacking',\n",
       " 'i',\n",
       " 'assume',\n",
       " 'that',\n",
       " 'there',\n",
       " 'is',\n",
       " 'some',\n",
       " 'workaround',\n",
       " 'using',\n",
       " 'css',\n",
       " 'styles',\n",
       " 'applied',\n",
       " 'to',\n",
       " 'table',\n",
       " 'cells',\n",
       " 'but',\n",
       " 'i',\n",
       " 'can',\n",
       " 'not',\n",
       " 'get',\n",
       " 'any',\n",
       " 'styles',\n",
       " 'to',\n",
       " 'apply',\n",
       " 'to',\n",
       " 'the',\n",
       " 'middle',\n",
       " 'table',\n",
       " 'cells',\n",
       " 'alas',\n",
       " 'its',\n",
       " 'beyond',\n",
       " 'me',\n",
       " 'at',\n",
       " 'the',\n",
       " 'moment',\n",
       " 'but',\n",
       " 'in',\n",
       " 'case',\n",
       " 'i',\n",
       " 'do',\n",
       " 'thats',\n",
       " 'why',\n",
       " 'ive',\n",
       " 'preserved',\n",
       " 'open',\n",
       " 'page',\n",
       " 'in',\n",
       " 'mozilla',\n",
       " 'or',\n",
       " 'firefox',\n",
       " 'the',\n",
       " 'page',\n",
       " 'doesnt',\n",
       " 'look',\n",
       " 'right',\n",
       " 'lack',\n",
       " 'of',\n",
       " 'top',\n",
       " 'vertical',\n",
       " 'alignment',\n",
       " 'and',\n",
       " 'lack',\n",
       " 'of',\n",
       " 'other',\n",
       " 'formatting',\n",
       " 'in',\n",
       " 'the',\n",
       " 'centre',\n",
       " 'content',\n",
       " 'table',\n",
       " 'made',\n",
       " 'it',\n",
       " 'look',\n",
       " 'right',\n",
       " 'all',\n",
       " 'columns',\n",
       " 'should',\n",
       " 'be',\n",
       " 'top',\n",
       " 'verticallyaligned']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_df['Description1_clean'].iloc[0]\n",
    "# data_df['Description2_clean'].iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'in the example page see url i have tried to vertically align all the cells using col valigntop  mac mozilla . and win firefox . both demonstrate this problem so it seems to be quite an old issue. it might be related to bug   though i dont believe that dealt with valign.  the page httptelcontar.netmiscplugins also shows that col wont apply background colours to cells either as well as not supporting valign  the whole left column is meant to be green but mozfirefox wont display the colour this  is possibly bug .  i have tried to use css instead using verticalalign instead of valign i tried applying styleverticalalign top to the col tags as a workaround in case this issue is due to a lack of support for outdated html . properties and this did not work either  col seems to not apply any formatting to cells whatsoever  neither css nor html ..  microsoft ie .. for the macintosh also displays the valign issue no support for either form of vertical alignment in col but icab b mac browser handles valign correctly not tried it with verticalalign as its css support is lacking.  i assume that there is some workaround using css styles applied to table cells but i cannot get any styles to apply to the middle table cells  alas its beyond me at the moment. but in case i do thats why ive preserved indexold.php    . open page in mozilla or firefox    the page doesnt look right. lack of top vertical alignment and lack of other formatting in the centre content table     made it look right. all columns should be top verticallyaligned'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_df['Description1'].iloc[0]\n",
    "# nondup_df['Description2'].iloc[0]\n",
    "# nondup_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def identify_tokens(row):\n",
    "    review = row['Title1']\n",
    "    tokens = nltk.word_tokenize(review)\n",
    "    token_words = [w for w in tokens if w.isalpha()]\n",
    "    return token_words\n",
    "\n",
    "data_df['Title1_clean'] = data_df.apply(identify_tokens, axis=1)\n",
    "nodup_df['Title1_clean'] = nodup_df.apply(identify_tokens, axis=1)\n",
    "\n",
    "def identify_tokens(row):\n",
    "    review = row['Title2']\n",
    "    tokens = nltk.word_tokenize(review)\n",
    "    token_words = [w for w in tokens if w.isalpha()]\n",
    "    return token_words\n",
    "\n",
    "data_df['Title2_clean'] = data_df.apply(identify_tokens, axis=1)\n",
    "nodup_df['Title2_clean'] = nodup_df.apply(identify_tokens, axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stops = set(stopwords.words(\"english\"))                  \n",
    "\n",
    "def remove_stops(row):\n",
    "    my_list = row['Description1_clean']\n",
    "    meaningful_words = [w for w in my_list if not w in stops]\n",
    "    return (meaningful_words)\n",
    "\n",
    "data_df['Description1_clean'] = data_df.apply(remove_stops, axis=1)\n",
    "nodup_df['Description1_clean'] = nodup_df.apply(remove_stops, axis=1)\n",
    "\n",
    "def remove_stops(row):\n",
    "    my_list = row['Description2_clean']\n",
    "    meaningful_words = [w for w in my_list if not w in stops]\n",
    "    return (meaningful_words)\n",
    "\n",
    "data_df['Description2_clean'] = data_df.apply(remove_stops, axis=1)\n",
    "nodup_df['Description2_clean'] = nodup_df.apply(remove_stops, axis=1)\n",
    "\n",
    "def remove_stops(row):\n",
    "    my_list = row['Title1_clean']\n",
    "    meaningful_words = [w for w in my_list if not w in stops]\n",
    "    return (meaningful_words)\n",
    "\n",
    "data_df['Title1_clean'] = data_df.apply(remove_stops, axis=1)\n",
    "nodup_df['Title1_clean'] = nodup_df.apply(remove_stops, axis=1)\n",
    "\n",
    "def remove_stops(row):\n",
    "    my_list = row['Title2_clean']\n",
    "    meaningful_words = [w for w in my_list if not w in stops]\n",
    "    return (meaningful_words)\n",
    "\n",
    "data_df['Title2_clean'] = data_df.apply(remove_stops, axis=1)\n",
    "nodup_df['Title2_clean'] = nodup_df.apply(remove_stops, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['example',\n",
       " 'page',\n",
       " 'see',\n",
       " 'url',\n",
       " 'tried',\n",
       " 'vertically',\n",
       " 'align',\n",
       " 'cells',\n",
       " 'using',\n",
       " 'col',\n",
       " 'valigntop',\n",
       " 'mac',\n",
       " 'mozilla',\n",
       " 'win',\n",
       " 'firefox',\n",
       " 'demonstrate',\n",
       " 'problem',\n",
       " 'seems',\n",
       " 'quite',\n",
       " 'old',\n",
       " 'issue',\n",
       " 'might',\n",
       " 'related',\n",
       " 'bug',\n",
       " 'though',\n",
       " 'dont',\n",
       " 'believe',\n",
       " 'dealt',\n",
       " 'valign',\n",
       " 'page',\n",
       " 'also',\n",
       " 'shows',\n",
       " 'col',\n",
       " 'wont',\n",
       " 'apply',\n",
       " 'background',\n",
       " 'colours',\n",
       " 'cells',\n",
       " 'either',\n",
       " 'well',\n",
       " 'supporting',\n",
       " 'valign',\n",
       " 'whole',\n",
       " 'left',\n",
       " 'column',\n",
       " 'meant',\n",
       " 'green',\n",
       " 'mozfirefox',\n",
       " 'wont',\n",
       " 'display',\n",
       " 'colour',\n",
       " 'possibly',\n",
       " 'bug',\n",
       " 'tried',\n",
       " 'use',\n",
       " 'css',\n",
       " 'instead',\n",
       " 'using',\n",
       " 'verticalalign',\n",
       " 'instead',\n",
       " 'valign',\n",
       " 'tried',\n",
       " 'applying',\n",
       " 'styleverticalalign',\n",
       " 'top',\n",
       " 'col',\n",
       " 'tags',\n",
       " 'workaround',\n",
       " 'case',\n",
       " 'issue',\n",
       " 'due',\n",
       " 'lack',\n",
       " 'support',\n",
       " 'outdated',\n",
       " 'html',\n",
       " 'properties',\n",
       " 'work',\n",
       " 'either',\n",
       " 'col',\n",
       " 'seems',\n",
       " 'apply',\n",
       " 'formatting',\n",
       " 'cells',\n",
       " 'whatsoever',\n",
       " 'neither',\n",
       " 'css',\n",
       " 'html',\n",
       " 'microsoft',\n",
       " 'ie',\n",
       " 'macintosh',\n",
       " 'also',\n",
       " 'displays',\n",
       " 'valign',\n",
       " 'issue',\n",
       " 'support',\n",
       " 'either',\n",
       " 'form',\n",
       " 'vertical',\n",
       " 'alignment',\n",
       " 'col',\n",
       " 'icab',\n",
       " 'b',\n",
       " 'mac',\n",
       " 'browser',\n",
       " 'handles',\n",
       " 'valign',\n",
       " 'correctly',\n",
       " 'tried',\n",
       " 'verticalalign',\n",
       " 'css',\n",
       " 'support',\n",
       " 'lacking',\n",
       " 'assume',\n",
       " 'workaround',\n",
       " 'using',\n",
       " 'css',\n",
       " 'styles',\n",
       " 'applied',\n",
       " 'table',\n",
       " 'cells',\n",
       " 'get',\n",
       " 'styles',\n",
       " 'apply',\n",
       " 'middle',\n",
       " 'table',\n",
       " 'cells',\n",
       " 'alas',\n",
       " 'beyond',\n",
       " 'moment',\n",
       " 'case',\n",
       " 'thats',\n",
       " 'ive',\n",
       " 'preserved',\n",
       " 'open',\n",
       " 'page',\n",
       " 'mozilla',\n",
       " 'firefox',\n",
       " 'page',\n",
       " 'doesnt',\n",
       " 'look',\n",
       " 'right',\n",
       " 'lack',\n",
       " 'top',\n",
       " 'vertical',\n",
       " 'alignment',\n",
       " 'lack',\n",
       " 'formatting',\n",
       " 'centre',\n",
       " 'content',\n",
       " 'table',\n",
       " 'made',\n",
       " 'look',\n",
       " 'right',\n",
       " 'columns',\n",
       " 'top',\n",
       " 'verticallyaligned']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_df['Description1_clean'].iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rejoin_words(row):\n",
    "    my_list = row['Description1_clean']\n",
    "    \n",
    "    # \" \".join() to the function to join the lists of words back together.\n",
    "    joined_words = ( \" \".join(my_list))\n",
    "    \n",
    "    return joined_words\n",
    "\n",
    "data_df['Description1_processed'] = data_df.apply(rejoin_words, axis=1)\n",
    "nodup_df['Description1_processed'] = nodup_df.apply(rejoin_words, axis=1)\n",
    "\n",
    "def rejoin_words(row):\n",
    "    my_list = row['Description2_clean']\n",
    "    \n",
    "    # \" \".join() to the function to join the lists of words back together.\n",
    "    joined_words = ( \" \".join(my_list))\n",
    "    \n",
    "    return joined_words\n",
    "\n",
    "data_df['Description2_processed'] = data_df.apply(rejoin_words, axis=1)\n",
    "nodup_df['Description2_processed'] = nodup_df.apply(rejoin_words, axis=1)\n",
    "\n",
    "def rejoin_words(row):\n",
    "    my_list = row['Title1_clean']\n",
    "    \n",
    "    # \" \".join() to the function to join the lists of words back together.\n",
    "    joined_words = ( \" \".join(my_list))\n",
    "    \n",
    "    return joined_words\n",
    "\n",
    "data_df['Title1_processed'] = data_df.apply(rejoin_words, axis=1)\n",
    "nodup_df['Title1_processed'] = nodup_df.apply(rejoin_words, axis=1)\n",
    "\n",
    "def rejoin_words(row):\n",
    "    my_list = row['Title2_clean']\n",
    "    \n",
    "    # \" \".join() to the function to join the lists of words back together.\n",
    "    joined_words = ( \" \".join(my_list))\n",
    "    \n",
    "    return joined_words\n",
    "\n",
    "data_df['Title2_processed'] = data_df.apply(rejoin_words, axis=1)\n",
    "nodup_df['Title2_processed'] = nodup_df.apply(rejoin_words, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'example page see url tried vertically align cells using col valigntop mac mozilla win firefox demonstrate problem seems quite old issue might related bug though dont believe dealt valign page also shows col wont apply background colours cells either well supporting valign whole left column meant green mozfirefox wont display colour possibly bug tried use css instead using verticalalign instead valign tried applying styleverticalalign top col tags workaround case issue due lack support outdated html properties work either col seems apply formatting cells whatsoever neither css html microsoft ie macintosh also displays valign issue support either form vertical alignment col icab b mac browser handles valign correctly tried verticalalign css support lacking assume workaround using css styles applied table cells get styles apply middle table cells alas beyond moment case thats ive preserved open page mozilla firefox page doesnt look right lack top vertical alignment lack formatting centre content table made look right columns top verticallyaligned'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_df['Description1_processed'].iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Processed</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>49137</th>\n",
       "      <td>flaky display paragraphs pictures located righ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49138</th>\n",
       "      <td>click open new tab longer gives last sites ope...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49139</th>\n",
       "      <td>every pagescreen change makes size jump large ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49140</th>\n",
       "      <td>page problems</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49141</th>\n",
       "      <td>allow restricted searches bookmarks history si...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49142</th>\n",
       "      <td>popup blocker randomly malfunctions</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49143</th>\n",
       "      <td>firefox stops working responding video goes ne...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49144</th>\n",
       "      <td>doesnt let type anything return tab faceboook ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49145</th>\n",
       "      <td>web ui bug</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49146</th>\n",
       "      <td>update crash</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49147</th>\n",
       "      <td>unable type apostrophe quicksearch arises</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49148</th>\n",
       "      <td>hovering elements bookmark toolbar shows blank...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49149</th>\n",
       "      <td>options tab hangs freezes</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49150</th>\n",
       "      <td>firefox responding</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49151</th>\n",
       "      <td>firefox running awhile autocomplete stops working</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Processed  Label\n",
       "49137  flaky display paragraphs pictures located righ...      0\n",
       "49138  click open new tab longer gives last sites ope...      0\n",
       "49139  every pagescreen change makes size jump large ...      0\n",
       "49140                                      page problems      0\n",
       "49141  allow restricted searches bookmarks history si...      0\n",
       "49142                popup blocker randomly malfunctions      0\n",
       "49143  firefox stops working responding video goes ne...      0\n",
       "49144  doesnt let type anything return tab faceboook ...      0\n",
       "49145                                         web ui bug      0\n",
       "49146                                       update crash      0\n",
       "49147          unable type apostrophe quicksearch arises      0\n",
       "49148  hovering elements bookmark toolbar shows blank...      0\n",
       "49149                          options tab hangs freezes      0\n",
       "49150                                 firefox responding      0\n",
       "49151  firefox running awhile autocomplete stops working      0"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "des1 = data_df.loc[data_df['Label'] == 1, ['Description1_clean', 'Description1_processed', 'Label']]\n",
    "des2 = data_df.loc[data_df['Label'] == 1, ['Description2_clean','Description2_processed', 'Label']]\n",
    "\n",
    "title1 = data_df.loc[data_df['Label'] == 1, ['Title1_clean', 'Title1_processed', 'Label']]\n",
    "title2 = data_df.loc[data_df['Label'] == 1, ['Title2_clean','Title2_processed', 'Label']]\n",
    "\n",
    "# Relabeling for appending correctly..\n",
    "des1.columns = des1.columns.str.replace('Description1_processed', 'Processed')\n",
    "des2.columns = des2.columns.str.replace('Description2_processed', 'Processed')\n",
    "title1.columns = title1.columns.str.replace('Title1_processed', 'Processed')\n",
    "title2.columns = title2.columns.str.replace('Title2_processed', 'Processed')\n",
    "\n",
    "\n",
    "mozilla_des_dup_out = des1.append(des2, ignore_index=True)\n",
    "mozilla_title_dup_out = title1.append(title2, ignore_index=True)\n",
    "dup_out = mozilla_des_dup_out.append(mozilla_title_dup_out, ignore_index=True)\n",
    "\n",
    "dup_out = dup_out[['Processed', 'Label']]\n",
    "# dup_out.to_csv('Final_df.csv')\n",
    "\n",
    "########################################\n",
    "des1 = nodup_df.loc[nodup_df['Label'] == 0, ['Description1_clean', 'Description1_processed', 'Label']]\n",
    "des2 = nodup_df.loc[nodup_df['Label'] == 0, ['Description2_clean', 'Description2_processed', 'Label']]\n",
    "\n",
    "title1 = nodup_df.loc[nodup_df['Label'] == 0, ['Title1_clean', 'Title1_processed', 'Label']]\n",
    "title2 = nodup_df.loc[nodup_df['Label'] == 0, ['Title2_clean','Title2_processed', 'Label']]\n",
    "\n",
    "## Relabeling for appending correctly..\n",
    "des1.columns = des1.columns.str.replace('Description1_processed', 'Processed')\n",
    "des2.columns = des2.columns.str.replace('Description2_processed', 'Processed')\n",
    "\n",
    "title1.columns = title1.columns.str.replace('Title1_processed', 'Processed')\n",
    "title2.columns = title2.columns.str.replace('Title2_processed', 'Processed')\n",
    "\n",
    "mozilla_des_nondup_out = des1.append(des2, ignore_index=True)\n",
    "mozilla_title_nondup_out = title1.append(title2, ignore_index=True)\n",
    "mozilla_nondup_out = mozilla_des_nondup_out.append(mozilla_title_nondup_out, ignore_index = True)\n",
    "\n",
    "mozilla_nondup_out = mozilla_nondup_out[['Processed', 'Label']]\n",
    "# nondup_out.to_csv('Final_df.csv')\n",
    "\n",
    "####### Combined Title and Des for Mozilla Dataset #######\n",
    "mozilla = dup_out.append(mozilla_nondup_out, ignore_index=True)\n",
    "\n",
    "mozilla['Label'] = mozilla['Label'].astype(int)\n",
    "\n",
    "mozilla.tail(15)\n",
    "# mozilla.to_csv('Mozilla_df.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Processed</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>dialup properties profile exposed prefs panels...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>would really like plugin manager browser allow...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>language encodings listed seemingly random ord...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>using synaptics touch pad latest win driver v ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>history window select scroll current site open...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>state checkbox dont allow removed cookies acce...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>build mac os reproduce open history window loo...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>since days search bookmarks gone search menu b...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>mozilla save respond posted form correctly ins...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>rightclicking back button list appears previou...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>localization problems bookmarks sorted menu hi...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>update quicktime solve bug similar cases need ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>rfe mozilla support x session management see x...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>trying look form history autocomplete accident...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>bookmark pointing host responding either shut ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                            Processed  Label\n",
       "0   dialup properties profile exposed prefs panels...      0\n",
       "1   would really like plugin manager browser allow...      0\n",
       "2   language encodings listed seemingly random ord...      0\n",
       "3   using synaptics touch pad latest win driver v ...      0\n",
       "4   history window select scroll current site open...      0\n",
       "5   state checkbox dont allow removed cookies acce...      0\n",
       "6   build mac os reproduce open history window loo...      0\n",
       "7   since days search bookmarks gone search menu b...      0\n",
       "8   mozilla save respond posted form correctly ins...      0\n",
       "9   rightclicking back button list appears previou...      0\n",
       "10  localization problems bookmarks sorted menu hi...      0\n",
       "11  update quicktime solve bug similar cases need ...      0\n",
       "12  rfe mozilla support x session management see x...      0\n",
       "13  trying look form history autocomplete accident...      0\n",
       "14  bookmark pointing host responding either shut ...      0"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mozilla.head(15)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Issue_id', 'Duplicated_issue', 'Title1', 'Description1', 'Title2', 'Description2', 'Label']\n"
     ]
    }
   ],
   "source": [
    "########## Load ThurderBird DataSet #################\n",
    "data_df = pd.read_csv('./data/ThunderBird/dup_TB.csv')\n",
    "data_df.head()\n",
    "print (list(data_df))\n",
    "\n",
    "data_df['Description1']=data_df['Description1'].fillna('').apply(str)\n",
    "data_df['Description2']=data_df['Description2'].fillna('').apply(str)\n",
    "data_df['Title1']=data_df['Title1'].fillna('').apply(str)\n",
    "data_df['Title2']=data_df['Title2'].fillna('').apply(str)\n",
    "\n",
    "# data_df['Label'] = (data_df['Label'] =='1.00').astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Issue_id', 'Duplicated_issue', 'Title1', 'Description1', 'Title2', 'Description2', 'Label']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0       using  m commercial builds  ts maybe sometng w...\n",
       "1       at present there is no conduit available for s...\n",
       "2         there are many users wch dont know what happ...\n",
       "3       in the mailnews file menu there is a save as s...\n",
       "4       when you compose a new message mozilla first c...\n",
       "                              ...                        \n",
       "9900      i installed thunderbird hoping i could impor...\n",
       "9901      when clicking in text mail not html on url w...\n",
       "9902      i am using gmail imap accounts. when i recei...\n",
       "9903      the problem appears with icedove the debian ...\n",
       "9904    these have been permafail on mac .  testunexpe...\n",
       "Name: Description1, Length: 9905, dtype: object"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nodup_df = pd.read_csv('./data/ThunderBird/Nondup_TB.csv',encoding='cp1252')\n",
    "nodup_df.head()\n",
    "print (list(nodup_df))\n",
    "\n",
    "nodup_df['Description1']=nodup_df['Description1'].fillna('').apply(str)\n",
    "nodup_df['Description2']=nodup_df['Description2'].fillna('').apply(str)\n",
    "nodup_df['Title1']=nodup_df['Title1'].fillna('').apply(str)\n",
    "nodup_df['Title2']=nodup_df['Title2'].fillna('').apply(str)\n",
    "\n",
    "nodup_df['Description1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def identify_tokens(row):\n",
    "    review = row['Description1']\n",
    "    tokens = nltk.word_tokenize(review)\n",
    "    token_words = [w for w in tokens if w.isalpha()]\n",
    "    return token_words\n",
    "\n",
    "data_df['Description1_clean'] = data_df.apply(identify_tokens, axis=1)\n",
    "nodup_df['Description1_clean'] = nodup_df.apply(identify_tokens, axis=1)\n",
    "\n",
    "def identify_tokens(row):\n",
    "    review = row['Description2']\n",
    "    tokens = nltk.word_tokenize(review)\n",
    "    token_words = [w for w in tokens if w.isalpha()]\n",
    "    return token_words\n",
    "\n",
    "data_df['Description2_clean'] = data_df.apply(identify_tokens, axis=1)\n",
    "nodup_df['Description2_clean'] = nodup_df.apply(identify_tokens, axis=1)\n",
    "\n",
    "def identify_tokens(row):\n",
    "    review = row['Title1']\n",
    "    tokens = nltk.word_tokenize(review)\n",
    "    token_words = [w for w in tokens if w.isalpha()]\n",
    "    return token_words\n",
    "\n",
    "data_df['Title1_clean'] = data_df.apply(identify_tokens, axis=1)\n",
    "nodup_df['Title1_clean'] = nodup_df.apply(identify_tokens, axis=1)\n",
    "\n",
    "def identify_tokens(row):\n",
    "    review = row['Title2']\n",
    "    tokens = nltk.word_tokenize(review)\n",
    "    token_words = [w for w in tokens if w.isalpha()]\n",
    "    return token_words\n",
    "\n",
    "data_df['Title2_clean'] = data_df.apply(identify_tokens, axis=1)\n",
    "nodup_df['Title2_clean'] = nodup_df.apply(identify_tokens, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stops = set(stopwords.words(\"english\"))                  \n",
    "\n",
    "def remove_stops(row):\n",
    "    my_list = row['Description1_clean']\n",
    "    meaningful_words = [w for w in my_list if not w in stops]\n",
    "    return (meaningful_words)\n",
    "\n",
    "data_df['Description1_clean'] = data_df.apply(remove_stops, axis=1)\n",
    "nodup_df['Description1_clean'] = nodup_df.apply(remove_stops, axis=1)\n",
    "\n",
    "def remove_stops(row):\n",
    "    my_list = row['Description2_clean']\n",
    "    meaningful_words = [w for w in my_list if not w in stops]\n",
    "    return (meaningful_words)\n",
    "\n",
    "data_df['Description2_clean'] = data_df.apply(remove_stops, axis=1)\n",
    "nodup_df['Description2_clean'] = nodup_df.apply(remove_stops, axis=1)\n",
    "\n",
    "def remove_stops(row):\n",
    "    my_list = row['Title1_clean']\n",
    "    meaningful_words = [w for w in my_list if not w in stops]\n",
    "    return (meaningful_words)\n",
    "\n",
    "data_df['Title1_clean'] = data_df.apply(remove_stops, axis=1)\n",
    "nodup_df['Title1_clean'] = nodup_df.apply(remove_stops, axis=1)\n",
    "\n",
    "def remove_stops(row):\n",
    "    my_list = row['Title2_clean']\n",
    "    meaningful_words = [w for w in my_list if not w in stops]\n",
    "    return (meaningful_words)\n",
    "\n",
    "data_df['Title2_clean'] = data_df.apply(remove_stops, axis=1)\n",
    "nodup_df['Title2_clean'] = nodup_df.apply(remove_stops, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rejoin_words(row):\n",
    "    my_list = row['Description1_clean']\n",
    "    \n",
    "    # \" \".join() to the function to join the lists of words back together.\n",
    "    joined_words = ( \" \".join(my_list))\n",
    "    \n",
    "    return joined_words\n",
    "\n",
    "data_df['Description1_processed'] = data_df.apply(rejoin_words, axis=1)\n",
    "nodup_df['Description1_processed'] = nodup_df.apply(rejoin_words, axis=1)\n",
    "\n",
    "def rejoin_words(row):\n",
    "    my_list = row['Description2_clean']\n",
    "    \n",
    "    # \" \".join() to the function to join the lists of words back together.\n",
    "    joined_words = ( \" \".join(my_list))\n",
    "    \n",
    "    return joined_words\n",
    "\n",
    "data_df['Description2_processed'] = data_df.apply(rejoin_words, axis=1)\n",
    "nodup_df['Description2_processed'] = nodup_df.apply(rejoin_words, axis=1)\n",
    "\n",
    "def rejoin_words(row):\n",
    "    my_list = row['Title1_clean']\n",
    "    \n",
    "    # \" \".join() to the function to join the lists of words back together.\n",
    "    joined_words = ( \" \".join(my_list))\n",
    "    \n",
    "    return joined_words\n",
    "\n",
    "data_df['Title1_processed'] = data_df.apply(rejoin_words, axis=1)\n",
    "nodup_df['Title1_processed'] = nodup_df.apply(rejoin_words, axis=1)\n",
    "\n",
    "def rejoin_words(row):\n",
    "    my_list = row['Title2_clean']\n",
    "    \n",
    "    # \" \".join() to the function to join the lists of words back together.\n",
    "    joined_words = ( \" \".join(my_list))\n",
    "    \n",
    "    return joined_words\n",
    "\n",
    "data_df['Title2_processed'] = data_df.apply(rejoin_words, axis=1)\n",
    "nodup_df['Title2_processed'] = nodup_df.apply(rejoin_words, axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Processed</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>57037</th>\n",
       "      <td>trunk thunderbird build process broken process...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57038</th>\n",
       "      <td>msmpengexe kicks immediately denial service th...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57039</th>\n",
       "      <td>testunexpectedfail testbuildxpcshellteststoolk...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57040</th>\n",
       "      <td>zip io error file directory release builds</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57041</th>\n",
       "      <td>error thisfolderdisplaytreeselection null file...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57042</th>\n",
       "      <td>testunexpectedfail testplugincrashingjs testpl...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57043</th>\n",
       "      <td>tb b recipient autocomplete dropdown shows bla...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57044</th>\n",
       "      <td>make possible run xpcshell tests locally</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57045</th>\n",
       "      <td>use fx australis tabs</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57046</th>\n",
       "      <td>shared themes pt move mailwindowcss code shared</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57047</th>\n",
       "      <td>using thunderbird esr latest keep getting noti...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57048</th>\n",
       "      <td>picture feed showing</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57049</th>\n",
       "      <td>add flagcolpng linux</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57050</th>\n",
       "      <td>replace undefined gpromptservice servicesprompt</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57051</th>\n",
       "      <td>open sidebar messes print preview compose window</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Processed  Label\n",
       "57037  trunk thunderbird build process broken process...      0\n",
       "57038  msmpengexe kicks immediately denial service th...      0\n",
       "57039  testunexpectedfail testbuildxpcshellteststoolk...      0\n",
       "57040         zip io error file directory release builds      0\n",
       "57041  error thisfolderdisplaytreeselection null file...      0\n",
       "57042  testunexpectedfail testplugincrashingjs testpl...      0\n",
       "57043  tb b recipient autocomplete dropdown shows bla...      0\n",
       "57044           make possible run xpcshell tests locally      0\n",
       "57045                              use fx australis tabs      0\n",
       "57046    shared themes pt move mailwindowcss code shared      0\n",
       "57047  using thunderbird esr latest keep getting noti...      0\n",
       "57048                               picture feed showing      0\n",
       "57049                               add flagcolpng linux      0\n",
       "57050    replace undefined gpromptservice servicesprompt      0\n",
       "57051   open sidebar messes print preview compose window      0"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "des1 = data_df.loc[data_df['Label'] == 1, ['Description1_clean', 'Description1_processed', 'Label']]\n",
    "des2 = data_df.loc[data_df['Label'] == 1, ['Description2_clean','Description2_processed', 'Label']]\n",
    "\n",
    "title1 = data_df.loc[data_df['Label'] == 1, ['Title1_clean', 'Title1_processed', 'Label']]\n",
    "title2 = data_df.loc[data_df['Label'] == 1, ['Title2_clean','Title2_processed', 'Label']]\n",
    "\n",
    "# Relabeling for appending correctly..\n",
    "des1.columns = des1.columns.str.replace('Description1_processed', 'Processed')\n",
    "des2.columns = des2.columns.str.replace('Description2_processed', 'Processed')\n",
    "title1.columns = title1.columns.str.replace('Title1_processed', 'Processed')\n",
    "title2.columns = title2.columns.str.replace('Title2_processed', 'Processed')\n",
    "\n",
    "\n",
    "thunderbird_des_dup_out = des1.append(des2, ignore_index=True)\n",
    "thunderbird_title_dup_out = title1.append(title2, ignore_index=True)\n",
    "dup_out = thunderbird_des_dup_out.append(thunderbird_title_dup_out, ignore_index=True)\n",
    "\n",
    "dup_out = dup_out[['Processed', 'Label']]\n",
    "# dup_out.to_csv('Final_df.csv')\n",
    "\n",
    "########################################\n",
    "des1 = nodup_df.loc[nodup_df['Label'] == 0, ['Description1_clean', 'Description1_processed', 'Label']]\n",
    "des2 = nodup_df.loc[nodup_df['Label'] == 0, ['Description2_clean', 'Description2_processed', 'Label']]\n",
    "\n",
    "title1 = nodup_df.loc[nodup_df['Label'] == 0, ['Title1_clean', 'Title1_processed', 'Label']]\n",
    "title2 = nodup_df.loc[nodup_df['Label'] == 0, ['Title2_clean','Title2_processed', 'Label']]\n",
    "\n",
    "## Relabeling for appending correctly..\n",
    "des1.columns = des1.columns.str.replace('Description1_processed', 'Processed')\n",
    "des2.columns = des2.columns.str.replace('Description2_processed', 'Processed')\n",
    "\n",
    "title1.columns = title1.columns.str.replace('Title1_processed', 'Processed')\n",
    "title2.columns = title2.columns.str.replace('Title2_processed', 'Processed')\n",
    "\n",
    "thunderbird_des_nondup_out = des1.append(des2, ignore_index=True)\n",
    "thunderbird_title_nondup_out = title1.append(title2, ignore_index=True)\n",
    "nondup_out = thunderbird_des_nondup_out.append(thunderbird_title_nondup_out, ignore_index = True)\n",
    "\n",
    "nondup_out = nondup_out[['Processed', 'Label']]\n",
    "# nondup_out.to_csv('Final_df.csv')\n",
    "\n",
    "#####\n",
    "thunderbird = dup_out.append(nondup_out, ignore_index=True)\n",
    "\n",
    "thunderbird['Label'] = thunderbird['Label'].astype(int)\n",
    "\n",
    "thunderbird.tail(15)\n",
    "\n",
    "## Uncomment this will auto-generated a csv for thunderbird dataset after stop words, lemmination method applied.\n",
    "# thunderbird.to_csv('thunderbird.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Issue_id', 'Duplicated_issue', 'Title1', 'Description1', 'Title2', 'Description2', 'Label']\n"
     ]
    }
   ],
   "source": [
    "data_df = pd.read_csv('./data/Eclipse/EP_dup.csv')\n",
    "data_df.head()\n",
    "print (list(data_df))\n",
    "\n",
    "data_df['Description1']=data_df['Description1'].fillna('').apply(str)\n",
    "data_df['Description2']=data_df['Description2'].fillna('').apply(str)\n",
    "data_df['Title1']=data_df['Title1'].fillna('').apply(str)\n",
    "data_df['Title2']=data_df['Title2'].fillna('').apply(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Issue_id', 'Duplicated_issue', 'Title1', 'Description1', 'Title2', 'Description2', 'Label']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0    setup a project that contains a .gif resource ...\n",
       "1    opening repository resource  open the default ...\n",
       "2    kmpm \\tthis pr about the deletion indicator in...\n",
       "3    become synchronized with some project in a rep...\n",
       "4    for gettingsetting the managed state of a reso...\n",
       "5    iresource.setlocal has problems. this method w...\n",
       "6    the platform is able to notify people that a r...\n",
       "7    with the current vcm api a repository adapter ...\n",
       "8    the implementation has to be changed because a...\n",
       "9    repository creationdeletion  team stream creat...\n",
       "Name: Description1, dtype: object"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nodup_df = pd.read_csv('./data/Eclipse/EP_nondup.csv',encoding='cp1252')\n",
    "nodup_df.head()\n",
    "print (list(nodup_df))\n",
    "\n",
    "nodup_df['Description1']=nodup_df['Description1'].fillna('').apply(str)\n",
    "nodup_df['Description2']=nodup_df['Description2'].fillna('').apply(str)\n",
    "nodup_df['Title1']=nodup_df['Title1'].fillna('').apply(str)\n",
    "nodup_df['Title2']=nodup_df['Title2'].fillna('').apply(str)\n",
    "\n",
    "nodup_df['Description1'].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def identify_tokens(row):\n",
    "    review = row['Description1']\n",
    "    tokens = nltk.word_tokenize(review)\n",
    "    token_words = [w for w in tokens if w.isalpha()]\n",
    "    return token_words\n",
    "\n",
    "data_df['Description1_clean'] = data_df.apply(identify_tokens, axis=1)\n",
    "nodup_df['Description1_clean'] = nodup_df.apply(identify_tokens, axis=1)\n",
    "\n",
    "def identify_tokens(row):\n",
    "    review = row['Description2']\n",
    "    tokens = nltk.word_tokenize(review)\n",
    "    token_words = [w for w in tokens if w.isalpha()]\n",
    "    return token_words\n",
    "\n",
    "data_df['Description2_clean'] = data_df.apply(identify_tokens, axis=1)\n",
    "nodup_df['Description2_clean'] = nodup_df.apply(identify_tokens, axis=1)\n",
    "\n",
    "def identify_tokens(row):\n",
    "    review = row['Title1']\n",
    "    tokens = nltk.word_tokenize(review)\n",
    "    token_words = [w for w in tokens if w.isalpha()]\n",
    "    return token_words\n",
    "\n",
    "data_df['Title1_clean'] = data_df.apply(identify_tokens, axis=1)\n",
    "nodup_df['Title1_clean'] = nodup_df.apply(identify_tokens, axis=1)\n",
    "\n",
    "def identify_tokens(row):\n",
    "    review = row['Title2']\n",
    "    tokens = nltk.word_tokenize(review)\n",
    "    token_words = [w for w in tokens if w.isalpha()]\n",
    "    return token_words\n",
    "\n",
    "data_df['Title2_clean'] = data_df.apply(identify_tokens, axis=1)\n",
    "nodup_df['Title2_clean'] = nodup_df.apply(identify_tokens, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stops = set(stopwords.words(\"english\"))                  \n",
    "\n",
    "def remove_stops(row):\n",
    "    my_list = row['Description1_clean']\n",
    "    meaningful_words = [w for w in my_list if not w in stops]\n",
    "    return (meaningful_words)\n",
    "\n",
    "data_df['Description1_clean'] = data_df.apply(remove_stops, axis=1)\n",
    "nodup_df['Description1_clean'] = nodup_df.apply(remove_stops, axis=1)\n",
    "\n",
    "def remove_stops(row):\n",
    "    my_list = row['Description2_clean']\n",
    "    meaningful_words = [w for w in my_list if not w in stops]\n",
    "    return (meaningful_words)\n",
    "\n",
    "data_df['Description2_clean'] = data_df.apply(remove_stops, axis=1)\n",
    "nodup_df['Description2_clean'] = nodup_df.apply(remove_stops, axis=1)\n",
    "\n",
    "def remove_stops(row):\n",
    "    my_list = row['Title1_clean']\n",
    "    meaningful_words = [w for w in my_list if not w in stops]\n",
    "    return (meaningful_words)\n",
    "\n",
    "data_df['Title1_clean'] = data_df.apply(remove_stops, axis=1)\n",
    "nodup_df['Title1_clean'] = nodup_df.apply(remove_stops, axis=1)\n",
    "\n",
    "def remove_stops(row):\n",
    "    my_list = row['Title2_clean']\n",
    "    meaningful_words = [w for w in my_list if not w in stops]\n",
    "    return (meaningful_words)\n",
    "\n",
    "data_df['Title2_clean'] = data_df.apply(remove_stops, axis=1)\n",
    "nodup_df['Title2_clean'] = nodup_df.apply(remove_stops, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rejoin_words(row):\n",
    "    my_list = row['Description1_clean']\n",
    "    \n",
    "    # \" \".join() to the function to join the lists of words back together.\n",
    "    joined_words = ( \" \".join(my_list))\n",
    "    \n",
    "    return joined_words\n",
    "\n",
    "data_df['Description1_processed'] = data_df.apply(rejoin_words, axis=1)\n",
    "nodup_df['Description1_processed'] = nodup_df.apply(rejoin_words, axis=1)\n",
    "\n",
    "def rejoin_words(row):\n",
    "    my_list = row['Description2_clean']\n",
    "    \n",
    "    # \" \".join() to the function to join the lists of words back together.\n",
    "    joined_words = ( \" \".join(my_list))\n",
    "    \n",
    "    return joined_words\n",
    "\n",
    "data_df['Description2_processed'] = data_df.apply(rejoin_words, axis=1)\n",
    "nodup_df['Description2_processed'] = nodup_df.apply(rejoin_words, axis=1)\n",
    "\n",
    "def rejoin_words(row):\n",
    "    my_list = row['Title1_clean']\n",
    "    \n",
    "    # \" \".join() to the function to join the lists of words back together.\n",
    "    joined_words = ( \" \".join(my_list))\n",
    "    \n",
    "    return joined_words\n",
    "\n",
    "data_df['Title1_processed'] = data_df.apply(rejoin_words, axis=1)\n",
    "nodup_df['Title1_processed'] = nodup_df.apply(rejoin_words, axis=1)\n",
    "\n",
    "def rejoin_words(row):\n",
    "    my_list = row['Title2_clean']\n",
    "    \n",
    "    # \" \".join() to the function to join the lists of words back together.\n",
    "    joined_words = ( \" \".join(my_list))\n",
    "    \n",
    "    return joined_words\n",
    "\n",
    "data_df['Title2_processed'] = data_df.apply(rejoin_words, axis=1)\n",
    "nodup_df['Title2_processed'] = nodup_df.apply(rejoin_words, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Processed</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>187613</th>\n",
       "      <td>workbench rcp application run correctly workbe...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>187614</th>\n",
       "      <td>ide eclipse repeatedly crashes</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>187615</th>\n",
       "      <td>editormgmtsplit editor provide commands main menu</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>187616</th>\n",
       "      <td>could require work around incompatibilities pa...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>187617</th>\n",
       "      <td>rework default debug perspective</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>187618</th>\n",
       "      <td>cbi enable automatic tests platform ui test pl...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>187619</th>\n",
       "      <td>perspectives alternative min max</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>187620</th>\n",
       "      <td>eclipse crashes frequently ubuntu</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>187621</th>\n",
       "      <td>losing text cursor view editor</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>187622</th>\n",
       "      <td>dialogs filtering files filteredresourcesselec...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>187623</th>\n",
       "      <td>learn report bug report</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>187624</th>\n",
       "      <td>projectdescription name properties dosent work</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>187625</th>\n",
       "      <td>failed related</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>187626</th>\n",
       "      <td>eclipse kepler often crash ubuntu jrockit</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>187627</th>\n",
       "      <td>gtklinux blank windows gtk</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Processed  Label\n",
       "187613  workbench rcp application run correctly workbe...      0\n",
       "187614                     ide eclipse repeatedly crashes      0\n",
       "187615  editormgmtsplit editor provide commands main menu      0\n",
       "187616  could require work around incompatibilities pa...      0\n",
       "187617                   rework default debug perspective      0\n",
       "187618  cbi enable automatic tests platform ui test pl...      0\n",
       "187619                   perspectives alternative min max      0\n",
       "187620                  eclipse crashes frequently ubuntu      0\n",
       "187621                     losing text cursor view editor      0\n",
       "187622  dialogs filtering files filteredresourcesselec...      0\n",
       "187623                            learn report bug report      0\n",
       "187624     projectdescription name properties dosent work      0\n",
       "187625                                     failed related      0\n",
       "187626          eclipse kepler often crash ubuntu jrockit      0\n",
       "187627                         gtklinux blank windows gtk      0"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "des1 = data_df.loc[data_df['Label'] == 1, ['Description1_clean', 'Description1_processed', 'Label']]\n",
    "des2 = data_df.loc[data_df['Label'] == 1, ['Description2_clean','Description2_processed', 'Label']]\n",
    "\n",
    "title1 = data_df.loc[data_df['Label'] == 1, ['Title1_clean', 'Title1_processed', 'Label']]\n",
    "title2 = data_df.loc[data_df['Label'] == 1, ['Title2_clean','Title2_processed', 'Label']]\n",
    "\n",
    "# Relabeling for appending correctly..\n",
    "des1.columns = des1.columns.str.replace('Description1_processed', 'Processed')\n",
    "des2.columns = des2.columns.str.replace('Description2_processed', 'Processed')\n",
    "title1.columns = title1.columns.str.replace('Title1_processed', 'Processed')\n",
    "title2.columns = title2.columns.str.replace('Title2_processed', 'Processed')\n",
    "\n",
    "eclipse_des_dup_out = des1.append(des2, ignore_index=True)\n",
    "eclipse_title_dup_out = title1.append(title2, ignore_index=True)\n",
    "dup_out = eclipse_des_dup_out.append(eclipse_title_dup_out, ignore_index=True)\n",
    "\n",
    "dup_out = dup_out[['Processed', 'Label']]\n",
    "# dup_out.to_csv('Final_df.csv')\n",
    "\n",
    "########################################\n",
    "des1 = nodup_df.loc[nodup_df['Label'] == 0, ['Description1_clean', 'Description1_processed', 'Label']]\n",
    "des2 = nodup_df.loc[nodup_df['Label'] == 0, ['Description2_clean', 'Description2_processed', 'Label']]\n",
    "\n",
    "title1 = nodup_df.loc[nodup_df['Label'] == 0, ['Title1_clean', 'Title1_processed', 'Label']]\n",
    "title2 = nodup_df.loc[nodup_df['Label'] == 0, ['Title2_clean','Title2_processed', 'Label']]\n",
    "\n",
    "## Relabeling for appending correctly..\n",
    "des1.columns = des1.columns.str.replace('Description1_processed', 'Processed')\n",
    "des2.columns = des2.columns.str.replace('Description2_processed', 'Processed')\n",
    "\n",
    "title1.columns = title1.columns.str.replace('Title1_processed', 'Processed')\n",
    "title2.columns = title2.columns.str.replace('Title2_processed', 'Processed')\n",
    "\n",
    "eclipse_des_nondup_out = des1.append(des2, ignore_index=True)\n",
    "eclipse_title_nondup_out = title1.append(title2, ignore_index=True)\n",
    "nondup_out = eclipse_des_nondup_out.append(eclipse_title_nondup_out, ignore_index = True)\n",
    "\n",
    "nondup_out = nondup_out[['Processed', 'Label']]\n",
    "# nondup_out.to_csv('Final_df.csv')\n",
    "\n",
    "#####\n",
    "eclipse = dup_out.append(nondup_out, ignore_index=True)\n",
    "\n",
    "eclipse['Label'] = eclipse['Label'].astype(int)\n",
    "\n",
    "eclipse.tail(15)\n",
    "\n",
    "# eclipse.to_csv('eclipse.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Processed</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>293817</th>\n",
       "      <td>workbench rcp application run correctly workbe...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>293818</th>\n",
       "      <td>ide eclipse repeatedly crashes</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>293819</th>\n",
       "      <td>editormgmtsplit editor provide commands main menu</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>293820</th>\n",
       "      <td>could require work around incompatibilities pa...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>293821</th>\n",
       "      <td>rework default debug perspective</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>293822</th>\n",
       "      <td>cbi enable automatic tests platform ui test pl...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>293823</th>\n",
       "      <td>perspectives alternative min max</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>293824</th>\n",
       "      <td>eclipse crashes frequently ubuntu</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>293825</th>\n",
       "      <td>losing text cursor view editor</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>293826</th>\n",
       "      <td>dialogs filtering files filteredresourcesselec...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>293827</th>\n",
       "      <td>learn report bug report</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>293828</th>\n",
       "      <td>projectdescription name properties dosent work</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>293829</th>\n",
       "      <td>failed related</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>293830</th>\n",
       "      <td>eclipse kepler often crash ubuntu jrockit</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>293831</th>\n",
       "      <td>gtklinux blank windows gtk</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                Processed  Label\n",
       "293817  workbench rcp application run correctly workbe...      0\n",
       "293818                     ide eclipse repeatedly crashes      0\n",
       "293819  editormgmtsplit editor provide commands main menu      0\n",
       "293820  could require work around incompatibilities pa...      0\n",
       "293821                   rework default debug perspective      0\n",
       "293822  cbi enable automatic tests platform ui test pl...      0\n",
       "293823                   perspectives alternative min max      0\n",
       "293824                  eclipse crashes frequently ubuntu      0\n",
       "293825                     losing text cursor view editor      0\n",
       "293826  dialogs filtering files filteredresourcesselec...      0\n",
       "293827                            learn report bug report      0\n",
       "293828     projectdescription name properties dosent work      0\n",
       "293829                                     failed related      0\n",
       "293830          eclipse kepler often crash ubuntu jrockit      0\n",
       "293831                         gtklinux blank windows gtk      0"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### Combined all three dataset for title and description field for training - 'final_out' ###\n",
    "two = mozilla.append(thunderbird, ignore_index=True)\n",
    "final_out = two.append(eclipse, ignore_index = True)\n",
    "final_out.tail(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean length of sentence: 27.40355373138392\n",
      "max length of sentence: 1734\n",
      "std dev length of sentence: 39.24831594509565\n"
     ]
    }
   ],
   "source": [
    "final_out['l'] = final_out['Processed'].apply(lambda x: len(str(x).split(' ')))\n",
    "print(\"mean length of sentence: \" + str(final_out.l.mean()))\n",
    "print(\"max length of sentence: \" + str(final_out.l.max()))\n",
    "print(\"std dev length of sentence: \" + str(final_out.l.std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment 1 : LR for Title & Description field on all three dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import naive_bayes \n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from scipy.sparse import random as sparse_random\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn import tree\n",
    "\n",
    "\n",
    "y = final_out['Label'].values\n",
    "\n",
    "X = final_out['Processed'].values\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state = 42)\n",
    "\n",
    "\n",
    "text_clf = Pipeline([('vect', CountVectorizer()),\n",
    "                     ('tfidf', TfidfTransformer()),\n",
    "                     ('clf', KNeighborsClassifier()),\n",
    "                     ])\n",
    "\n",
    "text_clf.fit(X_train, y_train)\n",
    "\n",
    "predicted = text_clf.predict(X_test)\n",
    "\n",
    "print(classification_report(y_test, predicted))\n",
    "\n",
    "\n",
    "text_clf = Pipeline([('vect', CountVectorizer()),\n",
    "                     ('tfidf', TfidfTransformer()),\n",
    "                     ('clf', MultinomialNB()),\n",
    "                     ])\n",
    "\n",
    "text_clf.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "predicted = text_clf.predict(X_test)\n",
    "\n",
    "print(classification_report(y_test, predicted))\n",
    "\n",
    "\n",
    "text_clf = Pipeline([('vect', CountVectorizer()),\n",
    "                     ('tfidf', TfidfTransformer()),\n",
    "                     ('clf', LinearSVC()),\n",
    "                     ])\n",
    "\n",
    "text_clf.fit(X_train, y_train)\n",
    "\n",
    "\n",
    "predicted = text_clf.predict(X_test)\n",
    "\n",
    "print(classification_report(y_test, predicted))\n",
    "\n",
    "\n",
    "text_clf = Pipeline([('vect', CountVectorizer()),\n",
    "                     ('tfidf', TfidfTransformer()),\n",
    "                     ('clf', LogisticRegression()),\n",
    "                     ])\n",
    "\n",
    "text_clf.fit(X_train, y_train)\n",
    "\n",
    "predicted = text_clf.predict(X_test)\n",
    "\n",
    "print(classification_report(y_test, predicted))\n",
    "\n",
    "## TF-IDF Vectorizer\n",
    "# count_vec = CountVectorizer(binary = False, stop_words = 'english')\n",
    "# count_vec.fit(sentence_train)\n",
    "\n",
    "# X_train = count_vec.transform(sentence_train)\n",
    "# X_test  = count_vec.transform(sentence_test)\n",
    "\n",
    "# classifier = LogisticRegression()\n",
    "# classifier.fit(X_train, y_train)\n",
    "\n",
    "# y_pred = classifier.predict(X_test)\n",
    "# print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Get the confusion matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment 2: CNN for Title & Description field on all three dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelBinarizer, LabelEncoder\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "from tensorflow import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Dropout\n",
    "from keras.preprocessing import text, sequence\n",
    "from keras import utils\n",
    "\n",
    "np.random.seed(100)\n",
    "train_size = int(len(final_out) * .8)\n",
    "train_posts = final_out['Processed'][:train_size]\n",
    "train_tags = final_out['Label'][:train_size]\n",
    "\n",
    "test_posts = final_out['Processed'][train_size:]\n",
    "test_tags = final_out['Label'][train_size:]\n",
    "\n",
    "max_words = 783\n",
    "tokenize = text.Tokenizer(num_words=max_words, char_level=False)\n",
    "tokenize.fit_on_texts(train_posts) # only fit on train\n",
    "\n",
    "x_train = tokenize.texts_to_matrix(train_posts)\n",
    "x_test = tokenize.texts_to_matrix(test_posts)\n",
    "\n",
    "encoder = LabelEncoder()\n",
    "encoder.fit(train_tags)\n",
    "y_train = encoder.transform(train_tags)\n",
    "y_test = encoder.transform(test_tags)\n",
    "\n",
    "num_classes = np.max(y_train) + 1\n",
    "y_train = utils.to_categorical(y_train, num_classes)\n",
    "y_test = utils.to_categorical(y_test, num_classes)\n",
    "\n",
    "\n",
    "# Inspect the dimenstions of our training and test data (this is helpful to debug)\n",
    "print('x_train shape:', x_train.shape)\n",
    "print('x_test shape:', x_test.shape)\n",
    "print('y_train shape:', y_train.shape)\n",
    "print('Testy shape:', y_test.shape)\n",
    "\n",
    "batch_size = 32\n",
    "epochs = 10\n",
    "\n",
    "# Build the model\n",
    "model = Sequential()\n",
    "model.add(Dense(512, input_shape=(max_words,)))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(num_classes))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "              \n",
    "history = model.fit(x_train, y_train,\n",
    "                    batch_size=batch_size,\n",
    "                    epochs=epochs,\n",
    "                    verbose=1,\n",
    "                    validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Evaluate the model\n",
    "loss, accuracy = model.evaluate(x_train, y_train, verbose=False)\n",
    "print(\"Training Accuracy: {:.4f}\".format(accuracy))\n",
    "loss, accuracy = model.evaluate(x_test, y_test, verbose=False)\n",
    "print(\"Testing Accuracy:  {:.4f}\".format(accuracy))\n",
    "\n",
    "# ## Double Check using classification report\n",
    "# from sklearn.metrics import classification_report\n",
    "\n",
    "# y_pred1 = model.predict(x_test)\n",
    "# # Convert predictions classes to one hot vectors \n",
    "# # Y_pred_classes = np.argmax(y_pred1,axis = 1) \n",
    "# y_pred = y_pred1 > 0.5\n",
    "\n",
    "# print(confusion_matrix(y_test, y_pred))\n",
    "# print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "history_dict = history.history\n",
    "\n",
    "acc = history_dict['accuracy']\n",
    "val_acc = history_dict['val_accuracy']\n",
    "\n",
    "epochs = range(1, len(acc) + 1)\n",
    "\n",
    "# \"bo\" is for \"blue dot\"\n",
    "plt.plot(epochs, acc, 'r', label='Training Accuracy')\n",
    "# b is for \"solid blue line\"\n",
    "plt.plot(epochs, val_acc, 'b', label='Validation Accuracy')\n",
    "plt.title('Training and validation Accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment 3 : MC - CNN for Title & Description on all three dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "import tensorflow\n",
    "from pickle import load\n",
    "from numpy import array\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils.vis_utils import plot_model\n",
    "from keras.models import Model\n",
    "from keras.layers import Input\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import Embedding\n",
    "from keras.layers.convolutional import Conv1D\n",
    "from keras.layers.convolutional import MaxPooling1D\n",
    "from keras.layers.merge import concatenate\n",
    "import pydotplus\n",
    "import keras.utils\n",
    "keras.utils.vis_utils.pydot = pydotplus\n",
    "\n",
    "from sklearn.preprocessing import LabelBinarizer, LabelEncoder\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "from tensorflow import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Dropout\n",
    "from keras.preprocessing import text, sequence\n",
    "from keras import utils\n",
    "\n",
    "# fit a tokenizer\n",
    "def create_tokenizer(lines):\n",
    "    tokenizer = Tokenizer()\n",
    "    tokenizer.fit_on_texts(lines)\n",
    "    return tokenizer\n",
    " \n",
    "# calculate the maximum document length\n",
    "def max_length(lines):\n",
    "    return max([len(s.split()) for s in lines])\n",
    " \n",
    "# encode a list of lines\n",
    "def encode_text(tokenizer, lines, length):\n",
    "    # integer encode\n",
    "    encoded = tokenizer.texts_to_sequences(lines)\n",
    "    # pad encoded sequences\n",
    "    padded = pad_sequences(encoded, maxlen=length, padding='post')\n",
    "    return padded\n",
    "\n",
    "def define_model(length, vocab_size):\n",
    "\t# channel 1\n",
    "\tinputs1 = tf.keras.layers.Input(shape=(length,))\n",
    "\tembedding1 = tf.keras.layers.Embedding(vocab_size, 100)(inputs1)\n",
    "\tconv1 = tf.keras.layers.Conv1D(filters=32, kernel_size=4, activation='relu')(embedding1)\n",
    "\tdrop1 = tf.keras.layers.Dropout(0.5)(conv1)\n",
    "\tpool1 = tf.keras.layers.MaxPooling1D(pool_size=2)(drop1)\n",
    "\tflat1 = tf.keras.layers.Flatten()(pool1)\n",
    "\t# channel 2\n",
    "\tinputs2 = tf.keras.layers.Input(shape=(length,))\n",
    "\tembedding2 = tf.keras.layers.Embedding(vocab_size, 100)(inputs2)\n",
    "\tconv2 = tf.keras.layers.Conv1D(filters=32, kernel_size=6, activation='relu')(embedding2)\n",
    "\tdrop2 = tf.keras.layers.Dropout(0.5)(conv2)\n",
    "\tpool2 = tf.keras.layers.MaxPooling1D(pool_size=2)(drop2)\n",
    "\tflat2 = tf.keras.layers.Flatten()(pool2)\n",
    "\t# channel 3\n",
    "\tinputs3 = tf.keras.layers.Input(shape=(length,))\n",
    "\tembedding3 = tf.keras.layers.Embedding(vocab_size, 100)(inputs3)\n",
    "\tconv3 = tf.keras.layers.Conv1D(filters=32, kernel_size=8, activation='relu')(embedding3)\n",
    "\tdrop3 = tf.keras.layers.Dropout(0.5)(conv3)\n",
    "\tpool3 = tf.keras.layers.MaxPooling1D(pool_size=2)(drop3)\n",
    "\tflat3 = tf.keras.layers.Flatten()(pool3)\n",
    "\t# merge\n",
    "\tmerged = tf.keras.layers.concatenate([flat1, flat2, flat3])\n",
    "\t# interpretation\n",
    "\tdense1 = tf.keras.layers.Dense(10, activation='relu')(merged)\n",
    "\toutputs = tf.keras.layers.Dense(1, activation='sigmoid')(dense1)\n",
    "\tmodel = tf.keras.models.Model(inputs=[inputs1, inputs2, inputs3], outputs=outputs)\n",
    "\t# compile\n",
    "\tmodel.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\t# summarize\n",
    "\tprint(model.summary())\n",
    "# \tplot_model(model, show_shapes=True, to_file='multichannel.png')\n",
    "\treturn model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max document length: 1734\n",
      "Vocabulary size: 192248\n",
      "(235065, 1734) (58767, 1734)\n",
      "(235065,) (235065,)\n",
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 1734)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            [(None, 1734)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_3 (InputLayer)            [(None, 1734)]       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding (Embedding)           (None, 1734, 100)    19224800    input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, 1734, 100)    19224800    input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "embedding_2 (Embedding)         (None, 1734, 100)    19224800    input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1d (Conv1D)                 (None, 1731, 32)     12832       embedding[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_1 (Conv1D)               (None, 1729, 32)     19232       embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv1d_2 (Conv1D)               (None, 1727, 32)     25632       embedding_2[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dropout (Dropout)               (None, 1731, 32)     0           conv1d[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_1 (Dropout)             (None, 1729, 32)     0           conv1d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "dropout_2 (Dropout)             (None, 1727, 32)     0           conv1d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d (MaxPooling1D)    (None, 865, 32)      0           dropout[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_1 (MaxPooling1D)  (None, 864, 32)      0           dropout_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling1d_2 (MaxPooling1D)  (None, 863, 32)      0           dropout_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "flatten (Flatten)               (None, 27680)        0           max_pooling1d[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)             (None, 27648)        0           max_pooling1d_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "flatten_2 (Flatten)             (None, 27616)        0           max_pooling1d_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 82944)        0           flatten[0][0]                    \n",
      "                                                                 flatten_1[0][0]                  \n",
      "                                                                 flatten_2[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 10)           829450      concatenate[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 1)            11          dense[0][0]                      \n",
      "==================================================================================================\n",
      "Total params: 58,561,557\n",
      "Trainable params: 58,561,557\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "919/919 [==============================] - 3038s 3s/step - loss: 0.5152 - accuracy: 0.7651\n"
     ]
    }
   ],
   "source": [
    "from keras.utils.vis_utils import plot_model\n",
    "from keras.layers import Dense, Dropout, Input, GlobalMaxPooling1D, Convolution1D, Embedding,SpatialDropout1D\n",
    "from keras import regularizers\n",
    "from keras.layers.merge import Concatenate\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import naive_bayes \n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "length = final_out.l.max()\n",
    "\n",
    "tokenizer = create_tokenizer(final_out['Processed'].values)\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "\n",
    "tokenizer = create_tokenizer(final_out['Processed'].values)\n",
    "\n",
    "length = max_length(final_out['Processed'].values)\n",
    "\n",
    "# calculate vocabulary size\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "print('Max document length: %d' % length)\n",
    "print('Vocabulary size: %d' % vocab_size) \n",
    "\n",
    "y = final_out['Label'].values\n",
    "\n",
    "X = final_out['Processed'].values\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state = 42)\n",
    "\n",
    "# # encode data\n",
    "trainX = encode_text(tokenizer, X_train, length)\n",
    "testX = encode_text(tokenizer, X_test, length)\n",
    "\n",
    "## Understand the shape before going into training..\n",
    "print(trainX.shape, testX.shape)\n",
    "print(X_train.shape, y_train.shape)\n",
    "\n",
    "# # define model\n",
    "model = define_model(length, vocab_size)\n",
    "\n",
    "# # fit model\n",
    "history = model.fit([trainX,trainX,trainX], array(y_train), epochs=1, batch_size=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'conv1' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-35-93dd4ac91dd1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconv1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'conv1' is not defined"
     ]
    }
   ],
   "source": [
    "print(conv1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(drop1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'mdoel' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-36-e8ee0e2c5342>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmdoel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'mdoel' is not defined"
     ]
    }
   ],
   "source": [
    "mdoel.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import models\n",
    "\n",
    "layer_outputs = [layer.output for layer in model.layers[:5]]\n",
    "activation_model = models.Model(inputs=model.input, outputs=layer_outputs)\n",
    "activations = activation_model.predict([trainX, trainX, trainX])\n",
    "first_layer_activation = activations[0]\n",
    "\n",
    "print(first_layer_activation.shape)\n",
    "\n",
    "plt.plot(first_layer_activation[0,:])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## Double Check using classification report\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "y_pred1 = model.predict([testX,testX,testX])\n",
    "# print(y_pred1.shape)\n",
    "# print(y_pred1[:50])\n",
    "y_pred_bool = np.argmax(y_pred1, axis=1)\n",
    "\n",
    "# print(classification_report(y_test, y_pred1))\n",
    "\n",
    "# Convert predictions classes to one hot vectors \n",
    "# Y_pred_classes = np.argmax(y_pred1,axis = 1) \n",
    "y_pred = y_pred1 > 0.5\n",
    "\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Evaluate the model\n",
    "loss, accuracy = model.evaluate([trainX,trainX,trainX], y_train, verbose=False)\n",
    "print(\"Training Accuracy: {:.4f}\".format(accuracy))\n",
    "loss, accuracy = model.evaluate([testX,testX,testX], y_test, verbose=False)\n",
    "print(\"Testing Accuracy:  {:.4f}\".format(accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now, Seperate Title and Description fields for training - MC-CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### MC-CNN -> Description Only for all three dataset ######\n",
    "mozilla_des_out = mozilla_des_dup_out.append(mozilla_des_nondup_out, ignore_index=True)\n",
    "thunderbird_des_out = thunderbird_des_dup_out.append(thunderbird_des_nondup_out, ignore_index=True)\n",
    "eclipse_des_out = eclipse_des_dup_out.append(eclipse_des_nondup_out, ignore_index=True)\n",
    "\n",
    "### Combined all three dataset only for description field for training labeled as 'final_out' ###\n",
    "two = mozilla_des_out.append(thunderbird_des_out, ignore_index=True)\n",
    "final_out = two.append(eclipse_des_out, ignore_index = True)\n",
    "final_out = final_out[['Processed', 'Label']]\n",
    "# final_out.head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_out['l'] = final_out['Processed'].apply(lambda x: len(str(x).split(' ')))\n",
    "print(\"mean length of sentence: \" + str(final_out.l.mean()))\n",
    "print(\"max length of sentence: \" + str(final_out.l.max()))\n",
    "print(\"std dev length of sentence: \" + str(final_out.l.std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "length = final_out.l.max()\n",
    "\n",
    "tokenizer = create_tokenizer(final_out['Processed'].values)\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "\n",
    "tokenizer = create_tokenizer(final_out['Processed'].values)\n",
    "\n",
    "length = max_length(final_out['Processed'].values)\n",
    "\n",
    "# calculate vocabulary size\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "print('Max document length: %d' % length)\n",
    "print('Vocabulary size: %d' % vocab_size) \n",
    "\n",
    "y = final_out['Label'].values\n",
    "\n",
    "X = final_out['Processed'].values\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state = 42)\n",
    "\n",
    "# # encode data\n",
    "trainX = encode_text(tokenizer, X_train, length)\n",
    "testX = encode_text(tokenizer, X_test, length)\n",
    "\n",
    "## Understand the shape before going into training..\n",
    "print(trainX.shape, testX.shape)\n",
    "print(X_train.shape, y_train.shape)\n",
    "\n",
    "# # define model\n",
    "model = define_model(length, vocab_size)\n",
    "\n",
    "# # fit model\n",
    "history = model.fit([trainX,trainX,trainX], array(y_train), epochs=3, batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred1 = model.predict([testX,testX,testX])\n",
    "# print(y_pred1.shape)\n",
    "# print(y_pred1[:50])\n",
    "\n",
    "# Convert predictions classes to one hot vectors \n",
    "# Y_pred_classes = np.argmax(y_pred1,axis = 1) \n",
    "y_pred = y_pred1 > 0.5\n",
    "\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Evaluate the model\n",
    "loss, accuracy = model.evaluate([trainX,trainX,trainX], y_train, verbose=False)\n",
    "print(\"Training Accuracy: {:.4f}\".format(accuracy))\n",
    "loss, accuracy = model.evaluate([testX,testX,testX], y_test, verbose=False)\n",
    "print(\"Testing Accuracy:  {:.4f}\".format(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### MC-CNN -> Title Only for all three dataset ######\n",
    "mozilla_title_out = mozilla_title_dup_out.append(mozilla_title_nondup_out, ignore_index=True)\n",
    "thunderbird_title_out = thunderbird_title_dup_out.append(thunderbird_title_nondup_out, ignore_index=True)\n",
    "eclipse_title_out = eclipse_title_dup_out.append(eclipse_title_nondup_out, ignore_index=True)\n",
    "\n",
    "### Combined all three dataset only for description field for training - 'final_out' ###\n",
    "two = mozilla_title_out.append(thunderbird_title_out, ignore_index=True)\n",
    "final_out = two.append(eclipse_title_out, ignore_index = True)\n",
    "final_out = final_out[['Processed', 'Label']]\n",
    "final_out.head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_out['l'] = final_out['Processed'].apply(lambda x: len(str(x).split(' ')))\n",
    "print(\"mean length of sentence: \" + str(final_out.l.mean()))\n",
    "print(\"max length of sentence: \" + str(final_out.l.max()))\n",
    "print(\"std dev length of sentence: \" + str(final_out.l.std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "length = final_out.l.max()\n",
    "\n",
    "tokenizer = create_tokenizer(final_out['Processed'].values)\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "\n",
    "tokenizer = create_tokenizer(final_out['Processed'].values)\n",
    "\n",
    "length = max_length(final_out['Processed'].values)\n",
    "\n",
    "# calculate vocabulary size\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "print('Max document length: %d' % length)\n",
    "print('Vocabulary size: %d' % vocab_size) \n",
    "\n",
    "y = final_out['Label'].values\n",
    "\n",
    "X = final_out['Processed'].values\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state = 42)\n",
    "\n",
    "# # encode data\n",
    "trainX = encode_text(tokenizer, X_train, length)\n",
    "testX = encode_text(tokenizer, X_test, length)\n",
    "\n",
    "## Understand the shape before going into training..\n",
    "print(trainX.shape, testX.shape)\n",
    "print(X_train.shape, y_train.shape)\n",
    "\n",
    "# # define model\n",
    "model = define_model(length, vocab_size)\n",
    "\n",
    "# # fit model\n",
    "history = model.fit([trainX,trainX,trainX], array(y_train), epochs=3, batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred1 = model.predict([testX,testX,testX])\n",
    "# print(y_pred1.shape)\n",
    "# print(y_pred1[:50])\n",
    "\n",
    "# Convert predictions classes to one hot vectors \n",
    "# Y_pred_classes = np.argmax(y_pred1,axis = 1) \n",
    "y_pred = y_pred1 > 0.5\n",
    "\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "## Evaluate the model\n",
    "loss, accuracy = model.evaluate([trainX,trainX,trainX], y_train, verbose=False)\n",
    "print(\"Training Accuracy: {:.4f}\".format(accuracy))\n",
    "loss, accuracy = model.evaluate([testX,testX,testX], y_test, verbose=False)\n",
    "print(\"Testing Accuracy:  {:.4f}\".format(accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiement 4: MC-CNN -> Combine Title and Description for Thunderbird dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mozilla_des_out = mozilla_des_dup_out.append(mozilla_des_nondup_out, ignore_index=True)\n",
    "mozilla_nondup_out = mozilla_title_dup_out.append(mozilla_title_nondup_out, ignore_index = True)\n",
    "final_out = mozilla_des_out.append(mozilla_nondup_out, ignore_index = True)\n",
    "\n",
    "final_out = final_out[['Processed', 'Label']]\n",
    "final_out.tail(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_out['l'] = final_out['Processed'].apply(lambda x: len(str(x).split(' ')))\n",
    "print(\"mean length of sentence: \" + str(final_out.l.mean()))\n",
    "print(\"max length of sentence: \" + str(final_out.l.max()))\n",
    "print(\"std dev length of sentence: \" + str(final_out.l.std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "length = final_out.l.max()\n",
    "\n",
    "tokenizer = create_tokenizer(final_out['Processed'].values)\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "\n",
    "tokenizer = create_tokenizer(final_out['Processed'].values)\n",
    "\n",
    "length = max_length(final_out['Processed'].values)\n",
    "\n",
    "# calculate vocabulary size\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "print('Max document length: %d' % length)\n",
    "print('Vocabulary size: %d' % vocab_size) \n",
    "\n",
    "y = final_out['Label'].values\n",
    "\n",
    "X = final_out['Processed'].values\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state = 42)\n",
    "\n",
    "# # encode data\n",
    "trainX = encode_text(tokenizer, X_train, length)\n",
    "testX = encode_text(tokenizer, X_test, length)\n",
    "\n",
    "## Understand the shape before going into training..\n",
    "print(trainX.shape, testX.shape)\n",
    "print(X_train.shape, y_train.shape)\n",
    "\n",
    "# # define model\n",
    "model = define_model(length, vocab_size)\n",
    "\n",
    "# # fit model\n",
    "history = model.fit([trainX,trainX,trainX], array(y_train), epochs=3, batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred1 = model.predict([testX,testX,testX])\n",
    "# print(y_pred1.shape)\n",
    "# print(y_pred1[:50])\n",
    "\n",
    "# Convert predictions classes to one hot vectors \n",
    "# Y_pred_classes = np.argmax(y_pred1,axis = 1) \n",
    "y_pred = y_pred1 > 0.5\n",
    "\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "## Evaluate the model\n",
    "loss, accuracy = model.evaluate([trainX,trainX,trainX], y_train, verbose=False)\n",
    "print(\"Training Accuracy: {:.4f}\".format(accuracy))\n",
    "loss, accuracy = model.evaluate([testX,testX,testX], y_test, verbose=False)\n",
    "print(\"Testing Accuracy:  {:.4f}\".format(accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiement 4: MC-CNN -> Combine Title and Description for Thunderbird dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thunderbird_des_out = thunderbird_des_dup_out.append(thunderbird_des_nondup_out, ignore_index=True)\n",
    "thunderbird_nondup_out = thunderbird_title_dup_out.append(thunderbird_title_nondup_out, ignore_index = True)\n",
    "final_out = thunderbird_des_out.append(thunderbird_nondup_out, ignore_index = True)\n",
    "\n",
    "final_out = final_out[['Processed', 'Label']]\n",
    "final_out.tail(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_out['l'] = final_out['Processed'].apply(lambda x: len(str(x).split(' ')))\n",
    "print(\"mean length of sentence: \" + str(final_out.l.mean()))\n",
    "print(\"max length of sentence: \" + str(final_out.l.max()))\n",
    "print(\"std dev length of sentence: \" + str(final_out.l.std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "length = final_out.l.max()\n",
    "\n",
    "tokenizer = create_tokenizer(final_out['Processed'].values)\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "\n",
    "tokenizer = create_tokenizer(final_out['Processed'].values)\n",
    "\n",
    "length = max_length(final_out['Processed'].values)\n",
    "\n",
    "# calculate vocabulary size\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "print('Max document length: %d' % length)\n",
    "print('Vocabulary size: %d' % vocab_size) \n",
    "\n",
    "y = final_out['Label'].values\n",
    "\n",
    "X = final_out['Processed'].values\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state = 42)\n",
    "\n",
    "# # encode data\n",
    "trainX = encode_text(tokenizer, X_train, length)\n",
    "testX = encode_text(tokenizer, X_test, length)\n",
    "\n",
    "## Understand the shape before going into training..\n",
    "print(trainX.shape, testX.shape)\n",
    "print(X_train.shape, y_train.shape)\n",
    "\n",
    "# # define model\n",
    "model = define_model(length, vocab_size)\n",
    "\n",
    "# # fit model\n",
    "history = model.fit([trainX,trainX,trainX], array(y_train), epochs=3, batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred1 = model.predict([testX,testX,testX])\n",
    "# print(y_pred1.shape)\n",
    "# print(y_pred1[:50])\n",
    "\n",
    "# Convert predictions classes to one hot vectors \n",
    "# Y_pred_classes = np.argmax(y_pred1,axis = 1) \n",
    "y_pred = y_pred1 > 0.5\n",
    "\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "## Evaluate the model\n",
    "loss, accuracy = model.evaluate([trainX,trainX,trainX], y_train, verbose=False)\n",
    "print(\"Training Accuracy: {:.4f}\".format(accuracy))\n",
    "loss, accuracy = model.evaluate([testX,testX,testX], y_test, verbose=False)\n",
    "print(\"Testing Accuracy:  {:.4f}\".format(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Experiement 4: MC-CNN -> Combine Title and Description for Eclipse dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eclipse_des_out = eclipse_des_dup_out.append(eclipse_des_nondup_out, ignore_index=True)\n",
    "eclipse_nondup_out = eclipse_title_dup_out.append(eclipse_title_nondup_out, ignore_index = True)\n",
    "final_out = eclipse_des_out.append(eclipse_nondup_out, ignore_index = True)\n",
    "\n",
    "final_out = final_out[['Processed', 'Label']]\n",
    "final_out.tail(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_out['l'] = final_out['Processed'].apply(lambda x: len(str(x).split(' ')))\n",
    "print(\"mean length of sentence: \" + str(final_out.l.mean()))\n",
    "print(\"max length of sentence: \" + str(final_out.l.max()))\n",
    "print(\"std dev length of sentence: \" + str(final_out.l.std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "length = final_out.l.max()\n",
    "\n",
    "tokenizer = create_tokenizer(final_out['Processed'].values)\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "\n",
    "tokenizer = create_tokenizer(final_out['Processed'].values)\n",
    "\n",
    "length = max_length(final_out['Processed'].values)\n",
    "\n",
    "# calculate vocabulary size\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "print('Max document length: %d' % length)\n",
    "print('Vocabulary size: %d' % vocab_size) \n",
    "\n",
    "y = final_out['Label'].values\n",
    "\n",
    "X = final_out['Processed'].values\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state = 42)\n",
    "\n",
    "# # encode data\n",
    "trainX = encode_text(tokenizer, X_train, length)\n",
    "testX = encode_text(tokenizer, X_test, length)\n",
    "\n",
    "## Understand the shape before going into training..\n",
    "print(trainX.shape, testX.shape)\n",
    "print(X_train.shape, y_train.shape)\n",
    "\n",
    "# # define model\n",
    "model = define_model(length, vocab_size)\n",
    "\n",
    "# # fit model\n",
    "history = model.fit([trainX,trainX,trainX], array(y_train), epochs=3, batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred1 = model.predict([testX,testX,testX])\n",
    "# print(y_pred1.shape)\n",
    "# print(y_pred1[:50])\n",
    "\n",
    "# Convert predictions classes to one hot vectors \n",
    "# Y_pred_classes = np.argmax(y_pred1,axis = 1) \n",
    "y_pred = y_pred1 > 0.5\n",
    "\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "## Evaluate the model\n",
    "loss, accuracy = model.evaluate([trainX,trainX,trainX], y_train, verbose=False)\n",
    "print(\"Training Accuracy: {:.4f}\".format(accuracy))\n",
    "loss, accuracy = model.evaluate([testX,testX,testX], y_test, verbose=False)\n",
    "print(\"Testing Accuracy:  {:.4f}\".format(accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#########################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
